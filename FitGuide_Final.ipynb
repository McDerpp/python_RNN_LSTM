{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA COLLECTING\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# cap = cv2.VideoCapture(\"vid4.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# PARAMETERS:\n",
    "# ================================================================================================================================\n",
    "# timer to ready before posing/getting ready for the excercise\n",
    "set_timer_start = 50\n",
    "\n",
    "# ================================================================================================================================\n",
    "# range of how much change will it be considered a movement\n",
    "# lower -> small movement and jittery flaw of mediapipe will be detected\n",
    "# higher -> it wont detect immediately important frames\n",
    "# 0-1\n",
    "change_range = .03\n",
    "\n",
    "# ================================================================================================================================\n",
    "# a counter for the amount of time for the wait time before performing the exercise again\n",
    "between_exercise_ctr = 6\n",
    "\n",
    "\n",
    "\n",
    "isInitiated = False\n",
    "exercise_exectuted_ctr=0\n",
    "prev_x = 0\n",
    "prev_y = 0\n",
    "no_movement_ctr = 0\n",
    "movement_boolean = False\n",
    "timer = 0\n",
    "no_movement_threshold = 32\n",
    "\n",
    "prev_array = []\n",
    "\n",
    "for x in range(33):\n",
    "    prev_array.append([0, 0])\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    timer += 1 \n",
    "    landmark_all_present = False\n",
    "\n",
    "    if timer == set_timer_start:\n",
    "        file = open('base1234.txt', 'w')\n",
    "        if isInitiated==False:\n",
    "            file.write(\"START\")\n",
    "            file.write('\\n')\n",
    "        isInitiated = True\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        ctr_check=0\n",
    "\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx == 32:\n",
    "                landmark_all_present = \"Landmarks -> ALL PRESENT\"\n",
    "            else:\n",
    "                landmark_all_present = \"Landmarks -> SOME ARE MISSING\"\n",
    "\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "\n",
    "#checks if no movement is detected-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "            if x >= (prev_array[idx][0] - change_range) and y <= (prev_array[idx][1] + change_range) and isInitiated:   \n",
    "                print(idx,\")\",x,\"---\",prev_array[idx][0],'---> NO MOVEMENT')\n",
    "                ctr_check+=1          \n",
    "                \n",
    "            else:\n",
    "                print(idx,\")\",x,\"---\",prev_array[idx][0],'--->YES MOVEMENT',)\n",
    "                movement_boolean = True\n",
    "                no_movement_ctr = 0\n",
    "\n",
    "            if ctr_check == no_movement_threshold:\n",
    "                no_movement_ctr += 1 \n",
    "\n",
    "            if no_movement_ctr>=25:\n",
    "                movement_boolean=False\n",
    "            prev_array[idx] = [x, y]\n",
    "        print('counter->',no_movement_ctr)\n",
    "        print(movement_boolean)\n",
    "\n",
    "#if movement is detected then it will record the coordinates into a text file-----------------------------------------------------------------------------------------------------\n",
    "        if movement_boolean:\n",
    "            text = \"MOVEMENT\"      \n",
    "            if timer >= set_timer_start and file is not None:\n",
    "                for landmark in prev_array:\n",
    "                    # file.write(\"<\")\n",
    "                    file.write(\"|\")\n",
    "                    file.write(str(landmark[0]))\n",
    "                    file.write(\"|\")\n",
    "                    file.write(str(landmark[1]))\n",
    "                    # file.write(\">\")\n",
    "                    # file.write(\"|\")\n",
    "                file.write('\\n')\n",
    "\n",
    "        if no_movement_ctr >= 5:    \n",
    "            movement_boolean = False\n",
    "            text = \"NO MOVEMENT!\"\n",
    "\n",
    "        if no_movement_ctr == between_exercise_ctr:\n",
    "            exercise_exectuted_ctr+=1\n",
    "            file.write(\"END\")  \n",
    "            file.write('\\n')\n",
    "            file.write('\\n')\n",
    "            file.write(\"START\")\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "        if timer >= set_timer_start and no_movement_ctr >=between_exercise_ctr:\n",
    "            cv2.putText(frame, \"PERFORM THE EXERCISE\", (180, 460), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Outputing important informations-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        cv2.putText(frame, str(movement_boolean), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"no_movement_ctr-> {no_movement_ctr}\", (20, 37), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, str(landmark_all_present), (20, 54), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"timer-> {timer}\", (20, 73), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"exercise exectuted-> {exercise_exectuted_ctr}\", (20, 92), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        if timer < set_timer_start:\n",
    "            cv2.putText(frame, \"PREPARE YOUR STARTING POSE\", (180, 460), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n",
    "            \n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "        )\n",
    "\n",
    "    cv2.imshow('MediaPipe Pose', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "if file is not None:\n",
    "    file.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def txt_pre_process(txt_file,label,simplify=False,simplify_level=4 ):\n",
    "    label_array = []\n",
    "    temp_feature_data = []\n",
    "    temp_sequence_data = []\n",
    "    batch_data = []\n",
    "\n",
    "    with open(str(txt_file), 'r') as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.strip().split('|')        \n",
    "\n",
    "            temp_feature_data = []\n",
    "\n",
    "            for value in values:            \n",
    "                float_value = str(value)\n",
    "\n",
    "                #FIRST PART OF THE SEQUENCE \n",
    "                if float_value == 'START':   \n",
    "                    temp_sequence_data=[]\n",
    "                                        \n",
    "                elif float_value == 'END':              \n",
    "                    batch_data.append(temp_sequence_data)\n",
    "                    label_array.append(label)\n",
    "\n",
    "            \n",
    "                elif float_value != '' and float_value != 'START': \n",
    "                    if simplify:    \n",
    "                        float_value = round(float(value),simplify_level)\n",
    "                    else:\n",
    "                        float_value = float(value)\n",
    "                    temp_feature_data.append(float_value)\n",
    "\n",
    "            if temp_feature_data!=[]:\n",
    "                temp_sequence_data.append(temp_feature_data)\n",
    "\n",
    "    label_array = np.array(label_array)\n",
    "    return [batch_data,label_array]\n",
    "\n",
    "#--------------------------------------------------------------------------- padding --------------------------------------------------------------------------------\n",
    "# padding can be improved probably...by using sequence \n",
    "# minor issue:\n",
    "# > is whether sequences had exceeded the intended number of sequences but is still right (it was performed right but slower(by an acceptable margin)) - not resolved\n",
    "#    = temporary fix was just to truncate everything if it had exceeded the intended number of sequence for the sake of running it for now\n",
    "#    = a reliable solution in theory could be that to randomly truncate in between the first and end sequence, in this way relevant data can be captured\n",
    "\n",
    "\n",
    "def padding(pre_processed_input,optional_maxLength=0): \n",
    "    padded_sequences = []\n",
    "    if optional_maxLength != 0:\n",
    "        max_length = optional_maxLength\n",
    "    else:\n",
    "        max_length = max(len(sequence) for sequence in pre_processed_input)\n",
    "    \n",
    "    for sequence in pre_processed_input:        \n",
    "        padding_length = max_length - len(sequence)\n",
    "        if padding_length >= 0:\n",
    "            padded_sequence = np.pad(sequence, ((0, padding_length), (0, 0)), mode='constant')\n",
    "            \n",
    "        else:\n",
    "            padded_sequence = sequence[:max_length]\n",
    "        padded_sequences.append(padded_sequence)\n",
    "    padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "#--------------------------------------------------------------------------- padding --------------------------------------------------------------------------------\n",
    "\n",
    "# this is to merge correct executions and wrong executions and randomize their input and label\n",
    "# positions of input and its corresponding label are the same \n",
    "def concatenate_randomize_batches(base_input,base_label,concat_input,concat_label):\n",
    "    combined_inputs = np.concatenate((base_input,concat_input), axis = 0)\n",
    "    combined_label = np.concatenate((base_label,concat_label), axis = 0)\n",
    "    indices = np.random.permutation(len(combined_inputs))\n",
    "    randomized_inputs = combined_inputs[indices]\n",
    "    randomized_label = combined_label[indices]\n",
    "    return [randomized_inputs,randomized_label]\n",
    "\n",
    "\n",
    "def predict_sequence(sequence_set,model,optional_maxLength=0):\n",
    "    loaded_model = tf.keras.models.load_model(model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_correct = txt_pre_process('base(correct_execution).txt',1,True,4)\n",
    "temp_wrong = txt_pre_process('base(wrong_execution).txt',0,True,4)\n",
    "\n",
    "base_input = padding(temp_correct[0])\n",
    "concat_input = padding(temp_wrong[0],31)\n",
    "\n",
    "base_label = temp_correct[1]\n",
    "concat_label = temp_wrong[1]\n",
    "\n",
    "# optional if labels were not initially assigned\n",
    "# correct_labels = np.ones(len(base_input))  # Assign label 1 to correct inputs\n",
    "# wrong_labels = np.zeros(len(concat_input))  # Assign label 0 to wrong inputs\n",
    "\n",
    "rand_batches=concatenate_randomize_batches(base_input,base_label,concat_input,concat_label)\n",
    "\n",
    "print(\"-------------------checking:-------------------\")\n",
    "print(\"base_input->\",base_input.shape)\n",
    "print(\"concat_input->\",concat_input.shape)\n",
    "print(\"successfully merged and randomized(input) ->\",rand_batches[0].shape)\n",
    "print(\"successfully merged and randomized(label) ->\",rand_batches[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2 - included a dropouts\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', dropout=0.1, recurrent_dropout=0.1, input_shape=(31, 66)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu')) \n",
    "model.add(LSTM(64, return_sequences=False, activation='relu')) \n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy', 'binary_accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# splits data into train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.1, random_state=42)\n",
    "\n",
    "# further splitting the dataset to have a set for simulation later on\n",
    "X_train, predict_simulate_input, y_train, predict_simulate_label = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model = tf.function(lambda x: model(x))\n",
    "# This is important, let's fix the input size.\n",
    "BATCH_SIZE = 1\n",
    "STEPS = 31\n",
    "INPUT_SIZE = 66\n",
    "concrete_func = run_model.get_concrete_function(\n",
    "    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
    "\n",
    "# model directory.\n",
    "MODEL_DIR = \"keras_lstm\"\n",
    "model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model with TensorFlow to get expected results.\n",
    "TEST_CASES = 10\n",
    "\n",
    "# Run the model with TensorFlow Lite\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "for i in range(TEST_CASES):\n",
    "  expected = model.predict(X_test[i:i+1])\n",
    "  interpreter.set_tensor(input_details[0][\"index\"], X_test[i:i+1, :, :])\n",
    "  interpreter.invoke()\n",
    "  result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "  # Assert if the result of TFLite model is consistent with the TF model.\n",
    "  np.testing.assert_almost_equal(expected, result, decimal=5)\n",
    "  print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
    "\n",
    "  # Please note: TfLite fused Lstm kernel is stateful, so we need to reset\n",
    "  # the states.\n",
    "  # Clean up internal states.\n",
    "  interpreter.reset_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_index = 0\n",
    "# sample_input = X_test[sample_index]\n",
    "\n",
    "# # Set the input tensor to the input data\n",
    "# interpreter.set_tensor(input_details[0]['index'], [sample_input])\n",
    "\n",
    "# # Perform inference\n",
    "# interpreter.invoke()\n",
    "\n",
    "# # Get the output tensor\n",
    "# output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# # Process the output to get the predicted class\n",
    "# predicted_class = np.argmax(output_data)\n",
    "# print(\"Predicted Class:\", predicted_class)\n",
    "# print(\"Predicted Class:\", output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction =[]\n",
    "one_exercise = []\n",
    "# max length of sequence\n",
    "for y in range(31):\n",
    "    sequence_input=[]\n",
    "    # max num of landmark x and y flattened\n",
    "    for x in range(66):\n",
    "        sequence_input.append(0)\n",
    "    one_exercise.append(sequence_input)\n",
    "\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# cap = cv2.VideoCapture(\"vid4.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        temp_sequence=[]\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            # print(idx,\")\",x,\"---\",prev_array[idx][0],)\n",
    "            # print(idx)\n",
    "            temp_sequence.append(x)\n",
    "            temp_sequence.append(y)\n",
    "\n",
    "    # if max length of the exercise sequence was reached then pop it pop off\n",
    "        if len(one_exercise) >= 31:\n",
    "            one_exercise.pop(0)\n",
    "\n",
    "\n",
    "        # one_exercise.append(temp_sequence)\n",
    "        # prediction = np.array(one_exercise)\n",
    "        # print(len(one_exercise))\n",
    "        # print(prediction.shape)\n",
    "        # prediction = prediction.reshape(1,31,66)\n",
    "        # print(model.predict(prediction))\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        frame,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "        connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "    )\n",
    "\n",
    "    cv2.imshow('MediaPipe Pose', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='converted_model.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
