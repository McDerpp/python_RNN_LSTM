{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> FLOW<h2>\n",
    "<h3>> Certain amount of time is set to give use some time to get ready</h3>\n",
    "<h3>> the trainer will then assume the pose that leads to the first part of the exercise</h3>\n",
    "<h3>> an indicator will prompt the user to perform the exercise</h3>\n",
    "<h3>> the user will have to stop in between exercise</h3>\n",
    "\n",
    "    example:    \n",
    "        push-up:    \n",
    "            1) Starting Position:\n",
    "            2) Push-Up Descent:\n",
    "            3) Push-Up Ascent:  \n",
    "            4) Stop momentarily, then it'll prompt to do it again   \n",
    "\n",
    "<h3>> every start of the exercise will be recorded unto a txt file for the landmark coordinates</h3>\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning game plan\n",
    ">collect data\n",
    ">apply first outlier detection, common sequences(sequences that has the most frequency)\n",
    ">apply second outlier detection, z-score\n",
    "\n",
    "cons for all of these approach\n",
    "> may delete vital data(but can be mitigated with careful determining of threshold)\n",
    "> reduces the dataset significantly(but can be solved through data augmentation)\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "data augmentation game plan\n",
    "> cleaned data is introduced to padding and truncating with noise instead of just 0\n",
    ">apply padding both post and pre(add pre-padding and post padding until the last sequence reaches the last indext of the max legnth) = this would simulate the constant stream of data and predicting(previous iteration was only paost padding resulting into total dependency whether the first sequence of the exercise is somewhere at the first index...this creates numerous problem.it would not predict as correct if the sequence is at the end portion of the index or in the middle in some cases)...adding noise would also (theoretically) make the model more robust\n",
    ">adding noises through feeding incorrect\n",
    "\n",
    ">we may also consider including the outlier that have more and less sequence(less sequence will be stretched out or the data will be disperesed with certain threshold this would mean that all data will be evenly distributed along the ideal number of sequence or the max sequence and add 0 or noise in between sequences to fill in the missing values...on the other hand, more sequences, we may opt to eliminate some of the data in between randomly until it reaches the max legnth or the ideal legnth)....however at the moment, i think this approach may create more problems in training due to the nature of its augmentations that may not totally represent the correct sequences(since we are already heavily augmenting it).\n",
    "\n",
    ">in general we may need to augment our data that simulate cases that may occur in inferencing\n",
    ">scenarios:\n",
    "    >certain vital part of the sequence may not have been capture due to some reason(frame skipped or we only collect data after every frame)\n",
    "    >constant stream of data being appended unto the array resulting to noise....supposed that: VS == vital seeuqnce and NS == noise sequence example-> [NS,NS,NS,VI,VI,VI,VI,NS,NS]\n",
    "    > vital sequence is in between noise sequences therefore the usual padding may not be sufficient and may not represent the kind of seuqnce under noise previous padding approach->[VS,VS,VS,VS,0,0,0,0] it has a total dependecy whether it the sequence is at the left most or the first index therfore it may not come up as a correct sequence if it have not arrived at the very first index.\n",
    "    >REMEMBER!-----------------------------> IT MUST NOT EXCEED a maximum of 50% of the total length since it might\n",
    "\n",
    "ADD A PURGING MECHANIC(DO NOT FORGET!)\n",
    "> it would clear the array if it had successfully predicted a correct\n",
    "> this would prevent situtaions wherein 2 correct sequences will be in the array\n",
    "\n",
    "\n",
    "\n",
    "problem encountered -> certain stance(if u dont move would be considered a correct or one exercise)\n",
    "> current hypothesis to this problem is that of how the data was augmented in the first place. the correct data was augmented to stride along the expanded length therefore now matter where the sequence is situated in the array, it will be considered correct, it also provides more flexibility and leeway. Model might have interpeted that commonality and pattern as all the sequence stride along the array.\n",
    "> this problem may be solved by simulating this scenario. through augmenting the correct data to have this scenario and claissfy as correct, this might be able to train the model to have this kind of scenario to be classified as falls.\n",
    "\n",
    "\n",
    "    >for instance : suppose array = [[1],[2],[3],[4],] will stride along an array with a length of 7\n",
    "    [0,0,0,[1],[2],[3],[4],]\n",
    "    [0,0,[1],[2],[3],[4],0]\n",
    "    [0,[1],[2],[3],[4],0,0,]\n",
    "    [[1],[2],[3],[4],0,0,0,]\n",
    "\n",
    "    model may interpret [,[1],[1],[1],[1],[1],[1],[1]] as correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import random as rand\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorflow_model_optimization.quantization.keras import quantize_model\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DATA COLLECTING\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "\n",
    "\n",
    "\n",
    "# mp_pose = mp.solutions.pose\n",
    "# pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # cap = cv2.VideoCapture(\"vid4.mp4\")\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# # PARAMETERS:\n",
    "# # ================================================================================================================================\n",
    "# # timer to ready before posing/getting ready for the excercise\n",
    "# set_timer_start = 50\n",
    "\n",
    "# # ================================================================================================================================\n",
    "# # range of how much change will it be considered a movement\n",
    "# # lower -> small movement and jittery flaw of mediapipe will be detected\n",
    "# # higher -> it wont detect immediately important frames\n",
    "# # 0-1\n",
    "# change_range = .03\n",
    "\n",
    "# # ================================================================================================================================\n",
    "# # a counter for the amount of time for the wait time before performing the exercise again\n",
    "# between_exercise_ctr = 10\n",
    "\n",
    "\n",
    "\n",
    "# isInitiated = False\n",
    "# exercise_exectuted_ctr=0\n",
    "# prev_x = 0\n",
    "# prev_y = 0\n",
    "# no_movement_ctr = 0\n",
    "# movement_boolean = False\n",
    "# timer = 0\n",
    "# no_movement_threshold = 32\n",
    "\n",
    "# prev_array = []\n",
    "\n",
    "\n",
    "# for x in range(33):\n",
    "#     prev_array.append([0, 0])\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = pose.process(image_rgb)\n",
    "\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     timer += 1 \n",
    "#     landmark_all_present = False\n",
    "\n",
    "#     if timer == set_timer_start:\n",
    "#         file = open('base1234.txt', 'w')\n",
    "#         if isInitiated==False:\n",
    "#             file.write(\"START\")\n",
    "#             file.write('\\n')\n",
    "#         isInitiated = True\n",
    "\n",
    "#     if results.pose_landmarks:\n",
    "#         ctr_check=0\n",
    "\n",
    "#         for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "#             if idx == 32:\n",
    "#                 landmark_all_present = \"Landmarks -> ALL PRESENT\"\n",
    "#             else:\n",
    "#                 landmark_all_present = \"Landmarks -> SOME ARE MISSING\"\n",
    "\n",
    "#             x = landmark.x\n",
    "#             y = landmark.y\n",
    "\n",
    "# #checks if no movement is detected-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "#             if x >= (prev_array[idx][0] - change_range) and y <= (prev_array[idx][1] + change_range) and isInitiated:   \n",
    "#                 print(idx,\")\",x,\"---\",prev_array[idx][0],'---> NO MOVEMENT')\n",
    "#                 ctr_check+=1          \n",
    "                \n",
    "#             else:\n",
    "#                 print(idx,\")\",x,\"---\",prev_array[idx][0],'--->YES MOVEMENT',)\n",
    "#                 movement_boolean = True\n",
    "#                 no_movement_ctr = 0\n",
    "\n",
    "#             if ctr_check == no_movement_threshold:\n",
    "#                 no_movement_ctr += 1 \n",
    "\n",
    "#             if no_movement_ctr>=25:\n",
    "#                 movement_boolean=False\n",
    "#             prev_array[idx] = [x, y]\n",
    "#         print('counter->',no_movement_ctr)\n",
    "#         print(movement_boolean)\n",
    "\n",
    "# #if movement is detected then it will record the coordinates into a text file-----------------------------------------------------------------------------------------------------\n",
    "#         if movement_boolean:\n",
    "#             text = \"MOVEMENT\"      \n",
    "#             if timer >= set_timer_start and file is not None:\n",
    "#                 for landmark in prev_array:\n",
    "#                     # file.write(\"<\")\n",
    "#                     file.write(\"|\")\n",
    "#                     file.write(str(landmark[0]))\n",
    "#                     file.write(\"|\")\n",
    "#                     file.write(str(landmark[1]))\n",
    "#                     # file.write(\">\")\n",
    "#                     # file.write(\"|\")\n",
    "#                 file.write('\\n')\n",
    "\n",
    "#         if no_movement_ctr >= 5:    \n",
    "#             movement_boolean = False\n",
    "#             text = \"NO MOVEMENT!\"\n",
    "\n",
    "#         if no_movement_ctr == between_exercise_ctr:\n",
    "#             exercise_exectuted_ctr+=1\n",
    "#             file.write(\"END\")  \n",
    "#             file.write('\\n')\n",
    "#             file.write('\\n')\n",
    "#             file.write(\"START\")\n",
    "#             file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#         if timer >= set_timer_start and no_movement_ctr >=between_exercise_ctr:\n",
    "#             cv2.putText(frame, \"PERFORM THE EXERCISE\", (180, 460), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n",
    "# # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# #Outputing important informations-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#         cv2.putText(frame, str(movement_boolean), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "#         cv2.putText(frame, f\"no_movement_ctr-> {no_movement_ctr}\", (20, 37), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "#         cv2.putText(frame, str(landmark_all_present), (20, 54), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "#         cv2.putText(frame, f\"timer-> {timer}\", (20, 73), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "#         cv2.putText(frame, f\"exercise exectuted-> {exercise_exectuted_ctr}\", (20, 92), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "# # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#         if timer < set_timer_start:\n",
    "#             cv2.putText(frame, \"PREPARE YOUR STARTING POSE\", (180, 460), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n",
    "            \n",
    "\n",
    "#         mp_drawing.draw_landmarks(\n",
    "#             frame,\n",
    "#             results.pose_landmarks,\n",
    "#             mp_pose.POSE_CONNECTIONS,\n",
    "#             landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "#             connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "#         )\n",
    "\n",
    "#     cv2.imshow('MediaPipe Pose', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# if file is not None:\n",
    "#     file.close()\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def txt_pre_process(txt_file,label,simplify=False,simplify_level=4 ):\n",
    "    label_array = []\n",
    "    temp_feature_data = []\n",
    "    temp_sequence_data = []\n",
    "    batch_data = []\n",
    "\n",
    "    with open(str(txt_file), 'r') as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.strip().split('|')        \n",
    "\n",
    "            temp_feature_data = []\n",
    "\n",
    "            for value in values:            \n",
    "                float_value = str(value)\n",
    "\n",
    "                #FIRST PART OF THE SEQUENCE \n",
    "                if float_value == 'START':   \n",
    "                    temp_sequence_data=[]\n",
    "                                        \n",
    "                elif float_value == 'END':              \n",
    "                    batch_data.append(temp_sequence_data)\n",
    "                    label_array.append(label)\n",
    "\n",
    "            \n",
    "                elif float_value != '' and float_value != 'START': \n",
    "                    if simplify:    \n",
    "                        float_value = round(float(value),simplify_level)\n",
    "                    else:\n",
    "                        float_value = float(value)\n",
    "                    temp_feature_data.append(float_value)\n",
    "\n",
    "            if temp_feature_data!=[]:\n",
    "                temp_sequence_data.append(temp_feature_data)\n",
    "\n",
    "    label_array = np.array(label_array)\n",
    "    return [batch_data,label_array]\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "# padding can be improved probably...by using sequence \n",
    "# minor issue:\n",
    "# > is whether sequences had exceeded the intended number of sequences but is still right (it was performed right but slower(by an acceptable margin)) - not resolved\n",
    "#    = temporary fix was just to truncate everything if it had exceeded the intended number of sequence for the sake of running it for now\n",
    "#    = a reliable solution in theory could be that to randomly truncate in between the first and end sequence, in this way relevant data can be captured\n",
    "def padding(pre_processed_input,optional_maxLength=0): \n",
    "    padded_sequences = []\n",
    "    if optional_maxLength != 0:\n",
    "        max_length = optional_maxLength\n",
    "    else:\n",
    "        max_length = max(len(sequence) for sequence in pre_processed_input)\n",
    "    \n",
    "    for sequence in pre_processed_input:        \n",
    "        padding_length = max_length - len(sequence)\n",
    "        if padding_length >= 0:\n",
    "            padded_sequence = np.pad(sequence, ((0, padding_length), (0, 0)), mode='constant')\n",
    "            \n",
    "        else:\n",
    "            padded_sequence = sequence[:max_length]\n",
    "        padded_sequences.append(padded_sequence)\n",
    "    padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "\n",
    "# this is to merge correct executions and wrong executions and randomize their input and label\n",
    "# positions of input and its corresponding label are the same \n",
    "# introducing noise/wrong input makes the model more robust\n",
    "def concatenate_randomize_batches(base_input,base_label,concat_input,concat_label):\n",
    "    combined_inputs = np.concatenate((base_input,concat_input), axis = 0)\n",
    "    combined_label = np.concatenate((base_label,concat_label), axis = 0)\n",
    "    indices = np.random.permutation(len(combined_inputs))\n",
    "    randomized_inputs = combined_inputs[indices]\n",
    "    randomized_label = combined_label[indices]\n",
    "    return [randomized_inputs,randomized_label]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# do not touch....for now...\n",
    "# this is for assigning of correct input and incorrect input for testing later\n",
    "def simulate_testing(dataset,index):   \n",
    "    input_data_simulate = dataset\n",
    "    max_sequence_length = 31\n",
    "\n",
    "    simulate_abrupt = 31\n",
    "    execution_index = index\n",
    "\n",
    "    testing123=input_data_simulate[execution_index].reshape((1, 31, 66))\n",
    "    temp69 = testing123[0][:simulate_abrupt]\n",
    "    temp69 = temp69.reshape((1, simulate_abrupt, 66))\n",
    "    temp69_padded = padding(temp69,31)\n",
    "    print(model.predict(temp69_padded))\n",
    "\n",
    "    correct_data =[]\n",
    "    wrong_data =[]\n",
    "    for x in range(len(predict_simulate_input)):\n",
    "        if predict_simulate_label[x] == 1:\n",
    "            correct_data.append(predict_simulate_input[x])\n",
    "            simulate_testing(predict_simulate_input,x)\n",
    "            print(\"CORRECT\")\n",
    "        else:\n",
    "            wrong_data.append(predict_simulate_input[x])\n",
    "            simulate_testing(predict_simulate_input,x)\n",
    "            print(\"WRONG\")\n",
    "\n",
    "    correct_data = np.array(correct_data)\n",
    "    wrong_data = np.array(wrong_data)\n",
    "    print(correct_data.shape)\n",
    "    print(wrong_data.shape) \n",
    "\n",
    "\n",
    "\n",
    "def tally_sequence(sequence_array):\n",
    "    tally_number = []\n",
    "    tally_ctr = []\n",
    "\n",
    "    for x in sequence_array:\n",
    "        temp = len(x)\n",
    "        if temp not in tally_number:\n",
    "            tally_number.append(temp)\n",
    "            tally_ctr.append(1)\n",
    "        else:\n",
    "            for y in range(len(tally_number)) :\n",
    "                if temp == tally_number[y]:\n",
    "                    tally_ctr[y] = tally_ctr[y] + 1\n",
    "\n",
    "    tally_max = 0\n",
    "    tally_number_arranged = []\n",
    "    tally_ctr_arranged = []\n",
    "\n",
    "    for x in range(len(tally_number)):\n",
    "        # print(len(tally_ctr)) \n",
    "        tally_max = max(tally_ctr)\n",
    "        for y in range(len(tally_number)):\n",
    "            if tally_ctr[y] == tally_max:\n",
    "                tally_number_arranged.append(tally_number[y])\n",
    "                tally_ctr_arranged.append(tally_ctr[y])                   \n",
    "                tally_ctr.pop(y)\n",
    "                tally_number.pop(y)\n",
    "                break\n",
    "\n",
    "    total_ctr = 0\n",
    "    for x in tally_ctr:\n",
    "        total_ctr = total_ctr + x\n",
    "\n",
    "\n",
    "    for x in range(len(tally_number_arranged)):\n",
    "        print(tally_number_arranged[x],'-->',tally_ctr_arranged[x])\n",
    "\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def common_length_sequence(sequences_array,threshold = 2):\n",
    "    temp = []\n",
    "\n",
    "    data = [len(seq) for seq in sequences_array]\n",
    "    data_frequency = Counter(data)\n",
    "    most_common_data = data_frequency.most_common()\n",
    "    outlier_frequencies = [value for value, freq in data_frequency.items() if freq < threshold]\n",
    "    most_common_values = [value for value, freq in most_common_data if freq >= threshold]\n",
    "\n",
    "    print(\"Most Common Data Points:\", most_common_values)\n",
    "    print(\"Outlier Frequencies:\", outlier_frequencies)\n",
    "\n",
    "    for x in sequences_array:        \n",
    "        if len(x) in most_common_values:\n",
    "            temp.append(x)\n",
    "    print('-------------------applied frequency outlier detection-------------------')\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(temp))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(temp))\n",
    "    return temp\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def apply_z_score(sequences_array,z_score_threshold = 1):\n",
    "    data_points = []\n",
    "    included_datapoints = []\n",
    "    updated_sequences =[]\n",
    "\n",
    "    for x in sequences_array:\n",
    "        temp = len(x)\n",
    "        if temp not in data_points:\n",
    "            data_points.append(temp)\n",
    "\n",
    "    data = np.array(data_points)\n",
    "    mean_value = np.mean(data)\n",
    "    standard_deviation = np.std(data)\n",
    "    z_scores = (data - mean_value) / standard_deviation\n",
    "    for x in range(len(z_scores)):\n",
    "        if np.abs(z_scores[x]) <= z_score_threshold:\n",
    "            included_datapoints.append(data[x])\n",
    "\n",
    "\n",
    "    for x in sequences_array:\n",
    "        if len(x) in included_datapoints:\n",
    "            updated_sequences.append(x)\n",
    "    print('-------------------applied z-score outlier detection-------------------')\n",
    "    print(\"datapoints included -> \", included_datapoints)\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(updated_sequences))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(updated_sequences))\n",
    "\n",
    "    return updated_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def paddingV2(sequences_array,optional_maxlength = 0):\n",
    "    output = []\n",
    "    max_length = 0\n",
    "    if optional_maxlength == 0:\n",
    "        max_length = max(len(sequence) for sequence in sequences_array)\n",
    "        expanded_max_length = int(max_length+ ((max_length) * .40))\n",
    "    else:\n",
    "        expanded_max_length = optional_maxlength\n",
    "\n",
    "    print(expanded_max_length)\n",
    "\n",
    "\n",
    "    padding_length_before = 0\n",
    "    padding_length_after = 0\n",
    "\n",
    "    for seq in sequences_array:\n",
    "        # print(seq)\n",
    "        for x in range(expanded_max_length-len(seq)+1):\n",
    "            padding_length_before = x\n",
    "            padding_length_after = expanded_max_length - len(seq) - x\n",
    "            padded_sequence = np.pad(seq, ((padding_length_before, padding_length_after),(0,0)), mode='constant')\n",
    "            output.append(padded_sequence)\n",
    "            \n",
    "            # print(padded_sequence)\n",
    "    print('------------------------applied paddingV2------------------------')\n",
    "    print('max_length -> ', max_length)\n",
    "    print('expanded_max_length -> ', expanded_max_length)\n",
    "    print('original num set of sequences -> ', len(sequences_array))\n",
    "    print('final num set of sequences -> ', len(output))\n",
    "\n",
    "    output = np.array(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def populate_0_input(correct_data,noise_data):\n",
    "    print(len(correct_data))\n",
    "    index = 10\n",
    "    temp = []\n",
    "    temp_compilation = []\n",
    "    ctr = 0\n",
    "    rand_modifier =0\n",
    "    \n",
    "    for set_sequence in correct_data:\n",
    "        rand_modifier = rand.randint(0,len(noise_data))\n",
    "\n",
    "        for x in range(len(set_sequence)):\n",
    "            ctr = ctr + 1 \n",
    "            if set_sequence[x][0] == 0:\n",
    "                temp.append(noise_data[rand_modifier-1][rand.randint(0,len(noise_data[rand_modifier-1])-1)])\n",
    "\n",
    "            else:\n",
    "                temp.append(set_sequence[x])\n",
    "\n",
    "        temp_compilation.append(temp)\n",
    "        temp =[]\n",
    "\n",
    "\n",
    "    return temp_compilation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_tf_to_tflite(tf_model):\n",
    "  model = tf.keras.models.load_model(tf_model)\n",
    "\n",
    "  run_model = tf.function(lambda x: model(x))\n",
    "  # This is important, let's fix the input size.\n",
    "  BATCH_SIZE = 1\n",
    "  STEPS = 31\n",
    "  INPUT_SIZE = 66\n",
    "  concrete_func = run_model.get_concrete_function(\n",
    "      tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
    "\n",
    "  # model directory.\n",
    "  MODEL_DIR = \"keras_lstm\"\n",
    "  model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
    "  \n",
    "  Converter.from_saved_model(MODEL_DIR)\n",
    "  tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "  # Run the model with TensorFlow to get expected results.\n",
    "  TEST_CASES = 10\n",
    "\n",
    "  # Run the model with TensorFlow Lite\n",
    "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "\n",
    "  for i in range(TEST_CASES):\n",
    "    expected = model.predict(X_test[i:i+1])\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], X_test[i:i+1, :, :])\n",
    "    interpreter.invoke()\n",
    "    result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "\n",
    "    np.testing.assert_almost_equal(expected, result, decimal=5)\n",
    "    print(\"it matches!!!!!!!!!.\")\n",
    "\n",
    "    interpreter.reset_all_variables()\n",
    "\n",
    "\n",
    "\n",
    "def model_inference(input,model,input_shape,input_details,output_details,name):\n",
    "    prediction = np.array(input)\n",
    "    prediction = prediction.astype(np.float32)\n",
    "    prediction = prediction.reshape(1,input_shape[1],input_shape[2])\n",
    "    model.set_tensor(input_details[0]['index'], prediction)\n",
    "    model.invoke()            \n",
    "    output_data = model.get_tensor(output_details[0]['index'])\n",
    "    print(name,'--->',output_data[0][0])\n",
    "    return(output_data[0][0])\n",
    "\n",
    "\n",
    "def draw_pose_lines(image,coordinates, color=(255, 0, 0), thickness=5):   \n",
    "\n",
    "    start_point = (int(coordinates[0] * image.shape[1]), \n",
    "                    int(coordinates[1] * image.shape[0]))\n",
    "    end_point = (int(coordinates[2] * image.shape[1]), \n",
    "                    int(coordinates[3]* image.shape[0]))\n",
    "\n",
    "    # Draw the line on the image\n",
    "    cv2.line(image, start_point, end_point, color, thickness)\n",
    "\n",
    "\n",
    "class CustomEarlyStopping(Callback):\n",
    "  def __init__(self, accuracy_threshold=0.95, loss_threshold=0.10):\n",
    "      super(CustomEarlyStopping, self).__init__()\n",
    "      self.accuracy_threshold = accuracy_threshold\n",
    "      self.loss_threshold = loss_threshold\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "      if logs is None:\n",
    "          logs = {}\n",
    "\n",
    "      if logs.get('val_accuracy') is None or logs.get('val_loss') is None:\n",
    "          return\n",
    "\n",
    "      if logs.get('val_accuracy') >= self.accuracy_threshold and logs.get('val_loss') <= self.loss_threshold:\n",
    "          self.model.stop_training = True\n",
    "          print(f\"\\nTraining stopped as validation accuracy reached {logs.get('val_accuracy'):.4f} \"\n",
    "                f\"and validation loss reached {logs.get('val_loss'):.4f}\")\n",
    "\n",
    "\n",
    "def aug_noise(sequences_array,optional_maxlength = 0):\n",
    "    output = []\n",
    "    temp = []\n",
    "    temp_array = []\n",
    "    temp_random_list = []\n",
    "    rand_temp = 0\n",
    "    max_length = 0\n",
    "    random_ctr = 0\n",
    "    new_sequence_array=[]\n",
    "    padding_length_before = 0\n",
    "    padding_length_after = 0\n",
    "\n",
    "    random_ctr_modifier = .5\n",
    "\n",
    "    if optional_maxlength == 0:\n",
    "        max_length = max(len(sequence) for sequence in sequences_array)\n",
    "        expanded_max_length = int(max_length+ ((max_length) * .40))\n",
    "    else:\n",
    "        expanded_max_length = optional_maxlength\n",
    "\n",
    "    random_ctr = int(len(sequences_array) *random_ctr_modifier)\n",
    "\n",
    "\n",
    "    for rand_num in range(random_ctr):\n",
    "        while rand_temp in temp_random_list:\n",
    "            rand_temp = rand.randint(1,len(sequences_array))-1\n",
    "        temp_random_list.append(rand_temp)\n",
    "\n",
    "    for x in temp_random_list:\n",
    "        new_sequence_array.append(sequences_array[x])\n",
    "\n",
    "# first noise/incorrrect generation(if you freeze at certain stance):\n",
    "    for set_of_sequence in new_sequence_array:\n",
    "        for one_sequence in set_of_sequence:\n",
    "            temp = []\n",
    "            for ctr in range(expanded_max_length):\n",
    "                temp.append(one_sequence)\n",
    "            temp_array.append(temp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is as a whole exercise\n",
    "# def streamlined_process(correct_execution,noise_data):\n",
    "#   base_data = txt_pre_process(correct_execution,1,False,4)\n",
    "#   base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
    "\n",
    "#   temp_correct_start=[]\n",
    "#   temp_correct_wrong=[]\n",
    "\n",
    "#   data = base_data[0]\n",
    "#   data_noise = base_data_noise[0]\n",
    "\n",
    "#   print(data[0])\n",
    "\n",
    "#   print('')\n",
    "#   print('----------------------------correct data augmentation ----------------------------')\n",
    "#   aug = common_length_sequence(data)\n",
    "#   aug2 = apply_z_score(aug,1)\n",
    "#   aug3 = paddingV2(aug2)\n",
    "#   aug4 = populate_0_input(aug3,data_noise)\n",
    "#   aug4 = np.array(aug4)\n",
    "#   aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
    "#   combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
    "#   combined_inputs = aug4\n",
    "#   print('concat -> ', len(combined_inputs))isInitiated = True\n",
    "\n",
    "\n",
    "#   print('')\n",
    "#   print('----------------------------data noise data augmentation ----------------------------')\n",
    "#   aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
    "#   aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
    "#   aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
    "#   aug_noise_data4 = np.array(aug_noise_data3)\n",
    "#   aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
    "\n",
    "\n",
    "#   aug3_label = np.ones(len(combined_inputs))\n",
    "#   aug_noise_label = np.zeros(len(aug_noise_data1))\n",
    "\n",
    "\n",
    "#   rand_batches=concatenate_randomize_batches(combined_inputs,aug3_label,aug_noise_data5,aug_noise_label)\n",
    "\n",
    "#   X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.3, random_state=42)\n",
    "\n",
    "# # basis in this version of model is the amount of sequences ( do not delete this! this is working!!!)--------------------------------------------------------------------------------------\n",
    "#   # model = Sequential()\n",
    "#   # model.add(Bidirectional(LSTM(len(aug3[0]), return_sequences=True, activation='relu',  input_shape=(len(aug3[0]), len(aug3[0][0])))))\n",
    "#   # model.add(Bidirectional(LSTM(len(aug3[0]) + int(len(aug3[0]) - int(len(aug3[0]) * .5)), return_sequences=True ,kernel_regularizer=l2(0.001),  activation='relu')))\n",
    "#   # model.add(Bidirectional(LSTM(len(aug3[0]) - int(len(aug3[0]) - int(len(aug3[0]) * .3)), return_sequences=True, dropout=0.1, recurrent_dropout=0.1, activation='relu')))\n",
    "#   # model.add(Bidirectional(LSTM(len(aug3[0]) - int(len(aug3[0]) - int(len(aug3[0]) * .5)), return_sequences=False, kernel_regularizer=l2(0.001), activation='relu')))\n",
    "#   # model.add(Dense(len(aug3[0]) - int(len(aug3[0]) - int(len(aug3[0]) * .5)), activation='relu'))\n",
    "#   # model.add(Dense(1,activation='sigmoid'))\n",
    "#   # model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy', 'binary_accuracy'])\n",
    "#   # model.fit(X_train, y_train, epochs=100, batch_size =128 , validation_data=(X_test, y_test))\n",
    "# # -------------------------------(DO NOT DELETE! THIS IS A WORKING ARCHITECTURE)-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# # yields better performance(DO NOT DELETE BETTER VERSION)-------------------------------------------------------------------------------\n",
    "#   model = Sequential()\n",
    "#   model.add(Bidirectional(LSTM(len(aug3[0]), return_sequences=True, activation='relu',  input_shape=(len(aug3[0]), len(aug3[0][0])))))\n",
    "#   model.add(Bidirectional(LSTM(len(aug3[0]) + int(len(aug3[0]) - int(len(aug3[0]) * .5)), return_sequences=True ,kernel_regularizer=l2(0.01),  activation='relu')))\n",
    "#   model.add(Bidirectional(LSTM(len(aug3[0]) - int(len(aug3[0]) - int(len(aug3[0]) * .2)), return_sequences=True, dropout=0.1, recurrent_dropout=0.1, activation='relu')))\n",
    "#   model.add(Bidirectional(LSTM(len(aug3[0]) - int(len(aug3[0]) - int(len(aug3[0]) * .5)), return_sequences=False, kernel_regularizer=l2(0.01), activation='relu')))\n",
    "#   model.add(Dense(len(aug3[0]) - int(len(aug3[0]) - int(len(aug3[0]) * .4)), activation='relu'))\n",
    "#   model.add(Dense(1,activation='sigmoid'))\n",
    "#   model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy', 'binary_accuracy'])\n",
    "#   model.fit(X_train, y_train, epochs=150, batch_size =128 , validation_data=(X_test, y_test))\n",
    "# # --------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   # basis in this version of model is the amount of features per sequences -----------------------------------------------------------------------------------------\n",
    "#   # model = Sequential()\n",
    "#   # model.add(Bidirectional(LSTM(len(aug3[0][0]), return_sequences=True, activation='relu',  input_shape=(len(aug3[0]), len(aug3[0][0])))))\n",
    "#   # model.add(Bidirectional(LSTM(len(aug3[0][0]) + int(len(aug3[0][0]) - int(len(aug3[0][0]) * .6)), return_sequences=True ,kernel_regularizer=l2(0.01),  activation='relu')))\n",
    "#   # model.add(Bidirectional(LSTM(len(aug3[0][0]) - int(len(aug3[0][0]) - int(len(aug3[0][0]) * .4)), return_sequences=True, dropout=0.1, recurrent_dropout=0.1, activation='relu')))\n",
    "#   # model.add(Bidirectional(LSTM(len(aug3[0][0]) - int(len(aug3[0][0]) - int(len(aug3[0][0]) * .6)), return_sequences=False, kernel_regularizer=l2(0.01), activation='relu')))\n",
    "#   # model.add(Dense(len(aug3[0][0]) - int(len(aug3[0]) - int(len(aug3[0][0]) * .6)), activation='relu'))\n",
    "#   # model.add(Dense(1,activation='sigmoid'))\n",
    "#   # model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy', 'binary_accuracy'])\n",
    "#   # model.fit(X_train, y_train, epochs=135, batch_size =128 , validation_data=(X_test, y_test))\n",
    "# # ----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#   model.save('testingModel')\n",
    "\n",
    "\n",
    "#   X_train = X_train.astype(np.float32)\n",
    "#   X_test = X_test.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# # update this....the parameter increased\n",
    "#   convert_tf_to_tflite('/content/testingModel',[1,len(aug3[0]),len(aug3[0][0])], X_test)\n",
    "\n",
    "\n",
    "\n",
    "#   # return(aug3,aug_noise)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # streamlined_process('base(correct_execution).txt','base(wrong_execution).txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0 1  -> 0\n",
    "# 2 3  -> 1\n",
    "# 4 5  -> 2\n",
    "# 6 7  -> 3\n",
    "# 8 9  -> 4\n",
    "# 10 11  -> 5\n",
    "# 12 13  -> 6\n",
    "# 14 15  -> 7\n",
    "# 16 17  -> 8\n",
    "# 18 19  -> 9\n",
    "# 20 21  -> 10\n",
    "# 22 23  -> 11\n",
    "# 24 25  -> 12\n",
    "# 26 27  -> 13\n",
    "# 28 29  -> 14\n",
    "# 30 31  -> 15\n",
    "# 32 33  -> 16\n",
    "# 34 35  -> 17\n",
    "# 36 37  -> 18\n",
    "# 38 39  -> 19\n",
    "# 40 41  -> 20\n",
    "# 42 43  -> 21\n",
    "# 44 45  -> 22\n",
    "# 46 47  -> 23\n",
    "# 48 49  -> 24\n",
    "# 50 51  -> 25\n",
    "# 52 53  -> 26\n",
    "# 54 55  -> 27\n",
    "# 56 57  -> 28\n",
    "# 58 59  -> 29\n",
    "# 60 61  -> 30\n",
    "# 62 63  -> 31\n",
    "# 64 65  -> 32\n",
    "\n",
    "# # ----------------------------------------------------------------------------\n",
    "#   left_upper_arm_sequence = []\n",
    "#   left_lower_arm_sequence = []\n",
    "#   left_hand_sequence = []\n",
    "\n",
    "#   right_upper_arm_sequence = []\n",
    "#   right_lower_arm_sequence = []\n",
    "#   right_hand_sequence = []\n",
    "\n",
    "#   left_upper_leg_sequence = []\n",
    "#   left_lower_leg_sequence = []\n",
    "#   left_feet_sequence = []\n",
    "\n",
    "#   right_upper_leg_sequence = []\n",
    "#   right_lower_leg_sequence = []\n",
    "#   right_feet_sequence = []\n",
    "\n",
    "#   body_sequence = []\n",
    "#   head_sequence = []\n",
    "\n",
    "#   # ------------------------------------------------------------------------------\n",
    "#   left_upper_arm_sequence_noise = []\n",
    "#   left_lower_arm_sequence_noise = []\n",
    "#   left_hand_sequence_noise = []\n",
    "\n",
    "#   right_upper_arm_sequence_noise = []\n",
    "#   right_lower_arm_sequence_noise = []\n",
    "#   right_hand_sequence_noise = []\n",
    "\n",
    "#   left_upper_leg_sequence_noise = []\n",
    "#   left_lower_leg_sequence_noise = []\n",
    "#   left_feet_sequence_noise = []\n",
    "\n",
    "#   right_upper_leg_sequence_noise = []\n",
    "#   right_lower_leg_sequence_noise = []\n",
    "#   right_feet_sequence_noise = []\n",
    "\n",
    "#   body_sequence_noise = []\n",
    "#   head_sequence_noise = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is for the individual models\n",
    "# def streamlined_process(correct_execution,noise_data):\n",
    "#   id_num = str(rand.randint(1000,9999))\n",
    "#   base_data = txt_pre_process(correct_execution,1,False,4)\n",
    "#   base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
    "\n",
    "#   temp_correct_start=[]\n",
    "#   temp_correct_wrong=[]\n",
    "\n",
    "#   data = base_data[0]\n",
    "#   data_noise = base_data_noise[0]\n",
    "\n",
    "\n",
    "\n",
    "#   print('')\n",
    "#   print('----------------------------correct data augmentation ----------------------------')\n",
    "#   aug = common_length_sequence(data)\n",
    "#   aug2 = apply_z_score(aug,1)\n",
    "#   aug3 = paddingV2(aug2)\n",
    "#   aug4 = populate_0_input(aug3,data_noise)\n",
    "#   aug4 = np.array(aug4)\n",
    "#   aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
    "#   combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
    "#   print('concat -> ', len(combined_inputs))\n",
    "\n",
    "\n",
    "#   print('')\n",
    "#   print('----------------------------data noise data augmentation ----------------------------')\n",
    "#   aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
    "#   aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
    "#   aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
    "#   aug_noise_data4 = np.array(aug_noise_data3)\n",
    "#   aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
    "\n",
    "\n",
    "# # initializing temp storage for data\n",
    "\n",
    "\n",
    "#   correct_data_set = []\n",
    "#   noise_data_set = []\n",
    "#   rand_batches = []\n",
    "\n",
    "\n",
    "#   for x in range(14):\n",
    "#     correct_data_set.append([])\n",
    "#     noise_data_set.append([])\n",
    "#     rand_batches.append([])\n",
    "\n",
    "\n",
    "#   for exercise in combined_inputs:\n",
    "\n",
    "\n",
    "#     left_upper_arm = []\n",
    "#     left_lower_arm = []\n",
    "#     left_hand = []\n",
    "#     right_upper_arm = []\n",
    "#     right_lower_arm = []\n",
    "#     right_hand = []\n",
    "#     left_upper_leg = []\n",
    "#     left_lower_leg = []\n",
    "#     left_feet = []\n",
    "#     right_upper_leg = []\n",
    "#     right_lower_leg = []\n",
    "#     right_feet = []\n",
    "#     head =[]\n",
    "#     body = []\n",
    "\n",
    "\n",
    "\n",
    "# #------------------------ generating the correct input of certain part------------------------------\n",
    "#     for sequence in exercise:\n",
    "#       # 11,13\n",
    "#       left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
    "#       # 13,15\n",
    "#       left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
    "#       # 15,17,19,21\n",
    "#       left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
    "\n",
    "#       # 12,14\n",
    "#       right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
    "#       # 14,16\n",
    "#       right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
    "#       # 16,18,20,22\n",
    "#       right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
    "\n",
    "#       # 23,25\n",
    "#       left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
    "#       # 25,27\n",
    "#       left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
    "#       # 27,29,31\n",
    "#       left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
    "\n",
    "#       # 24,26\n",
    "#       right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
    "#       # 26,28\n",
    "#       right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
    "#       # 28,30,32\n",
    "#       right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
    "\n",
    "#       # 11,12,23,24\n",
    "#       body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
    "#       # 7,8,9,10\n",
    "#       head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
    "\n",
    "\n",
    "#     correct_data_set[0].append(left_upper_arm)\n",
    "#     correct_data_set[1].append(left_lower_arm)\n",
    "#     correct_data_set[2].append(left_hand)\n",
    "\n",
    "#     correct_data_set[3].append(right_upper_arm)\n",
    "#     correct_data_set[4].append(right_lower_arm)\n",
    "#     correct_data_set[5].append(right_hand)\n",
    "\n",
    "#     correct_data_set[6].append(left_upper_leg)\n",
    "#     correct_data_set[7].append(left_lower_leg)\n",
    "#     correct_data_set[8].append(left_feet)\n",
    "\n",
    "#     correct_data_set[9].append(right_upper_leg)\n",
    "#     correct_data_set[10].append(right_lower_leg)\n",
    "#     correct_data_set[11].append(right_feet)\n",
    "\n",
    "#     correct_data_set[12].append(body)\n",
    "#     correct_data_set[13].append(head)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   for exercise in aug_noise_data5:\n",
    "\n",
    "\n",
    "#     left_upper_arm = []\n",
    "#     left_lower_arm = []\n",
    "#     left_hand = []\n",
    "#     right_upper_arm = []\n",
    "#     right_lower_arm = []\n",
    "#     right_hand = []\n",
    "#     left_upper_leg = []\n",
    "#     left_lower_leg = []\n",
    "#     left_feet = []\n",
    "#     right_upper_leg = []\n",
    "#     right_lower_leg = []\n",
    "#     right_feet = []\n",
    "#     head =[]\n",
    "#     body = []\n",
    "\n",
    "\n",
    "# #------------------------------ generating noise ---------------------------------------\n",
    "#     for sequence in exercise:\n",
    "#       # 11,13\n",
    "#       # print(sequence[22])\n",
    "#       left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
    "#       # 13,15\n",
    "#       left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
    "#       # 15,17,19,21\n",
    "#       left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
    "\n",
    "#       # 12,14\n",
    "#       right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
    "#       # 14,16\n",
    "#       right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
    "#       # 16,18,20,22\n",
    "#       right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
    "\n",
    "#       # 23,25\n",
    "#       left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
    "#       # 25,27\n",
    "#       left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
    "#       # 27,29,31\n",
    "#       left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
    "\n",
    "#       # 24,26\n",
    "#       right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
    "#       # 26,28\n",
    "#       right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
    "#       # 28,30,32\n",
    "#       right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
    "\n",
    "#       # 11,12,23,24\n",
    "#       body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
    "#       # 7,8,9,10\n",
    "#       head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
    "\n",
    "\n",
    "#     noise_data_set[0].append(left_upper_arm)\n",
    "#     noise_data_set[1].append(left_lower_arm)\n",
    "#     noise_data_set[2].append(left_hand)\n",
    "\n",
    "#     noise_data_set[3].append(right_upper_arm)\n",
    "#     noise_data_set[4].append(right_lower_arm)\n",
    "#     noise_data_set[5].append(right_hand)\n",
    "\n",
    "#     noise_data_set[6].append(left_upper_leg)\n",
    "#     noise_data_set[7].append(left_lower_leg)\n",
    "#     noise_data_set[8].append(left_feet)\n",
    "\n",
    "#     noise_data_set[9].append(right_upper_leg)\n",
    "#     noise_data_set[10].append(right_lower_leg)\n",
    "#     noise_data_set[11].append(right_feet)\n",
    "\n",
    "#     noise_data_set[12].append(body)\n",
    "#     noise_data_set[13].append(head)\n",
    "\n",
    "\n",
    "#   for x in range(len(correct_data_set)):\n",
    "#     correct_data_set[x] = np.array(correct_data_set[x])\n",
    "\n",
    "#   for x in range(len(noise_data_set)):\n",
    "#     noise_data_set[x] = np.array(noise_data_set[x])\n",
    "\n",
    "\n",
    "#   correct_data_set_label = np.ones(len(combined_inputs))\n",
    "#   noise_data_set_label = np.zeros(len(aug_noise_data1))\n",
    "\n",
    "#   for x in range(len(correct_data_set)):\n",
    "#     rand_batches[x]=concatenate_randomize_batches(correct_data_set[x],correct_data_set_label,noise_data_set[x],noise_data_set_label)\n",
    "\n",
    "#     # left_upper_arm = []\n",
    "#     # left_lower_arm = []\n",
    "#     # left_hand = []\n",
    "\n",
    "#     # right_upper_arm = []\n",
    "#     # right_lower_arm = []\n",
    "#     # right_hand = []\n",
    "\n",
    "#     # left_upper_leg = []\n",
    "#     # left_lower_leg = []\n",
    "#     # left_feet = []\n",
    "\n",
    "#     # right_upper_leg = []\n",
    "#     # right_lower_leg = []\n",
    "#     # right_feet = []\n",
    "\n",
    "#     # head =[]\n",
    "#     # body = []\n",
    "\n",
    "\n",
    "#   data_set_name=['left_upper_arm','left_lower_arm','left_hand','right_upper_arm','right_lower_arm','right_hand','left_upper_leg','left_lower_leg','left_feet','right_upper_leg','right_lower_leg','right_feet','head','body']\n",
    "#   for x in range(len(data_set_name)-1):\n",
    "#     print('progress -> ',x,'/',len(rand_batches[0][0]) )\n",
    "\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(rand_batches[x][0], rand_batches[x][1], test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "#     # model = Sequential()\n",
    "#     # model.add(Bidirectional(LSTM(len(X_train[0]), return_sequences=True, activation='relu',  input_shape=(len(X_train[0]), len(X_train[0][0])))))\n",
    "#     # model.add(Bidirectional(LSTM(len(X_train[0]) + int(len(X_train[0]) - int(len(X_train[0]) * .5)), return_sequences=True ,kernel_regularizer=l2(0.001),  activation='relu')))\n",
    "#     # model.add(Bidirectional(LSTM(len(X_train[0]) - int(len(X_train[0]) - int(len(X_train[0]) * .3)), return_sequences=True, dropout=0.1, recurrent_dropout=0.1, activation='relu')))\n",
    "#     # model.add(Bidirectional(LSTM(len(X_train[0]) - int(len(X_train[0]) - int(len(X_train[0]) * .5)), return_sequences=False, kernel_regularizer=l2(0.001), activation='relu')))\n",
    "#     # model.add(Dense(len(X_train[0]) - int(len(X_train[0]) - int(len(X_train[0]) * .5)), activation='relu'))\n",
    "#     # model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(len(X_train[0]), return_sequences=True, activation='relu',  input_shape=(len(X_train[0]), len(X_train[0][0]))))\n",
    "#     model.add(LSTM(len(X_train[0]) + int(len(X_train[0]) - int(len(X_train[0]) * .6)), return_sequences=True,  activation='relu'))\n",
    "#     model.add(Bidirectional(LSTM(len(X_train[0]) - int(len(X_train[0]) - int(len(X_train[0]) * .4)), return_sequences=True, dropout=0.5, recurrent_dropout=0.5, activation='relu')))\n",
    "#     model.add(LSTM(len(X_train[0]) - int(len(X_train[0]) - int(len(X_train[0]) * .5)), return_sequences=False,  activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(len(X_train[0]) - int(len(X_train[0]) - int(len(X_train[0]) * .4)), activation='relu'))\n",
    "#     model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#     custom_early_stopping = CustomEarlyStopping(accuracy_threshold=0.95, loss_threshold=0.07)\n",
    "#     model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy', 'binary_accuracy'])\n",
    "#     history = model.fit(X_train, y_train, epochs=200, batch_size =128 , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
    "\n",
    "\n",
    "#     val_loss = min(history.history['val_loss'])\n",
    "#     val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "#     # model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy', 'binary_accuracy'])\n",
    "#     # model.fit(X_train, y_train, epochs=150, batch_size =128 , validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     model.save('testingModel')\n",
    "\n",
    "\n",
    "#     X_train = X_train.astype(np.float32)\n",
    "#     X_test = X_test.astype(np.float32)\n",
    "#     convert_tf_to_tflite('/content/testingModel',[1,len(X_train[0]),len(X_train[0][0])], X_test,data_set_name[x],id_num,val_loss,val_accuracy)\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlined_process('/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt','/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_data = txt_pre_process('base(correct_execution).txt',1,True,4)\n",
    "# base_data_noise = txt_pre_process('base(wrong_execution).txt',0,True,4)\n",
    "\n",
    "# data = base_data\n",
    "# data_noise = base_data_noise\n",
    "\n",
    "# print('')\n",
    "# print('----------------------------correct data augmentation ----------------------------')\n",
    "# aug = common_length_sequence(data)\n",
    "# aug2 = apply_z_score(aug,1)\n",
    "# aug3 = paddingV2(aug2)\n",
    "\n",
    "# print('')\n",
    "# print('----------------------------data noise data augmentation ----------------------------')\n",
    "# aug_noise = paddingV2(data_noise,len(aug3[0]))\n",
    "# aug_noise = aug_noise[0:len(aug3)]\n",
    "\n",
    "\n",
    "# aug3_label = np.ones(len(aug3))\n",
    "# aug_noise_label = np.zeros(len(aug_noise))\n",
    "\n",
    "\n",
    "# rand_batches=concatenate_randomize_batches(aug3,aug3_label,aug_noise,aug_noise_label)\n",
    "# print('')\n",
    "# print('----------------------------final num set of sequences ----------------------------')\n",
    "# print('correct data ->',len(aug3))\n",
    "# print('wrong data ->',len(aug_noise))\n",
    "# print('combined and randomized data ->',rand_batches[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.1, random_state=42)\n",
    "\n",
    "# # further splitting the dataset to have a set for simulation later on\n",
    "# X_train, predict_simulate_input, y_train, predict_simulate_label = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True, activation='relu', dropout=0.1, recurrent_dropout=0.1, input_shape=(len(aug3[0]), len(aug3[0][0]))))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu')) \n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu')) \n",
    "# model.add(Dense(64, activation='relu')) \n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(1,activation='sigmoid'))\n",
    "# model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy', 'binary_accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------CORRECT EXECUTION WITH SIMULATION OF ABRUPT STOP-----------------------\n",
    "# # this is CORRECT EXECUTION but we will simulte a situation where the user will abruptly stop his/her execution\n",
    "# # 31 is the max length of sequence - therefore each execution is 31 length\n",
    "\n",
    "# # this variable simulates stoping at certain sequence\n",
    "\n",
    "# execution_index = 1\n",
    "# input_data_simulate = correct_data\n",
    "# max_sequence_length = len(input_data_simulate[0])\n",
    "# ctr = 0\n",
    "\n",
    "# print(\"len of sequence\",len(input_data_simulate[execution_index]))\n",
    "\n",
    "# for x in correct_data[execution_index]:\n",
    "#     ctr += 1\n",
    "#     if x[0]==0:\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "# for x in range(max_sequence_length):\n",
    "#     simulate_abrupt = x\n",
    "# # choose between 2-30 and simulate abrupt stop or simulate where a user might stop in between execution\n",
    "\n",
    "\n",
    "#     print(x,\"/\", ctr)\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     testing123=input_data_simulate[execution_index].reshape((1, 31, 66))\n",
    "#     temp69 = testing123[0][:simulate_abrupt]\n",
    "#     temp69 = temp69.reshape((1, simulate_abrupt, 66))\n",
    "#     temp69_padded = padding(temp69,31)\n",
    "#     print(model.predict(temp69_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------WRONG EXECUTION WITH SIMULATION OF ABRUPT STOP-----------------------\n",
    "# # this is WRONG EXECUTION but we will simulte a situation where the user will abruptly stop his/her execution\n",
    "# # 31 is the max length of sequence - therefore each execution is 31 length\n",
    "\n",
    "# # this variable simulates stoping at certain sequence\n",
    "# input_data_simulate = wrong_data\n",
    "# max_sequence_length = 31\n",
    "\n",
    "# # choose between 2-30 and simulate abrupt stop or simulate where a user might stop in between execution\n",
    "# simulate_abrupt = 31\n",
    "# execution_index = 2\n",
    "\n",
    "# testing123=input_data_simulate[execution_index].reshape((1, 31, 66))\n",
    "# temp69 = testing123[0][:simulate_abrupt]\n",
    "# temp69 = temp69.reshape((1, simulate_abrupt, 66))\n",
    "# temp69_padded = padding(temp69,31)\n",
    "# model.predict(temp69_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_model = tf.function(lambda x: model(x))\n",
    "# # This is important, let's fix the input size.\n",
    "# BATCH_SIZE = 1\n",
    "# STEPS = 31\n",
    "# INPUT_SIZE = 66\n",
    "# concrete_func = run_model.get_concrete_function(\n",
    "#     tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
    "\n",
    "# # model directory.\n",
    "# MODEL_DIR = \"keras_lstm\"\n",
    "# model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
    "# tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the model with TensorFlow to get expected results.\n",
    "# TEST_CASES = 10\n",
    "\n",
    "# # Run the model with TensorFlow Lite\n",
    "# interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "# interpreter.allocate_tensors()\n",
    "# input_details = interpreter.get_input_details()\n",
    "# output_details = interpreter.get_output_details()\n",
    "\n",
    "# for i in range(TEST_CASES):\n",
    "#   expected = model.predict(X_test[i:i+1])\n",
    "#   interpreter.set_tensor(input_details[0][\"index\"], X_test[i:i+1, :, :])\n",
    "#   interpreter.invoke()\n",
    "#   result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "#   # Assert if the result of TFLite model is consistent with the TF model.\n",
    "#   np.testing.assert_almost_equal(expected, result, decimal=5)\n",
    "#   print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
    "\n",
    "#   # Please note: TfLite fused Lstm kernel is stateful, so we need to reset\n",
    "#   # the states.\n",
    "#   # Clean up internal states.\n",
    "#   interpreter.reset_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inferencing\n",
    "# mp_pose = mp.solutions.pose\n",
    "# pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # cap = cv2.VideoCapture(\"vid4.mp4\")\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# frame_ctr = 0\n",
    "# frame_ctr_threshold = 0\n",
    "# exercise_ctr = 0\n",
    "# prev_prediction = 0\n",
    "# output_data = 0\n",
    "# temp_checking = []\n",
    "# all_array = []\n",
    "# # it means it is waiting to reset\n",
    "# prediction_buffer_threshold = .05\n",
    "# model = tf.lite.Interpreter(model_path=\"D:\\CLARK\\Documents\\RNN\\RNN_proofOfConcept\\converted_model8.tflite\")\n",
    "# model.allocate_tensors()\n",
    "# input_details = model.get_input_details()\n",
    "# output_details = model.get_output_details()\n",
    "# input_shape = input_details[0]['shape']\n",
    "# # print('input ->',)\n",
    "\n",
    "# prediction =[]\n",
    "# one_exercise = []\n",
    "# # max length of sequence\n",
    "# for y in range(input_shape[1]):\n",
    "#     sequence_input=[]\n",
    "#     # max num of landmark x and y flattened\n",
    "#     for x in range(66):\n",
    "#         sequence_input.append(0)\n",
    "#     one_exercise.append(sequence_input)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     frame_ctr += 1\n",
    "#     # print(frame_ctr)\n",
    "#     ret, frame = cap.read()\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = pose.process(image_rgb)\n",
    "\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     if results.pose_landmarks:\n",
    "#         temp_sequence=[]\n",
    "#         for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "#             x = landmark.x\n",
    "#             y = landmark.y\n",
    "#             # print(idx,\")\",x,\"---\",prev_array[idx][0],)\n",
    "#             # print(idx)\n",
    "#             x = round(float(x),4)\n",
    "#             y = round(float(y),4)\n",
    "#             temp_sequence.append(x)\n",
    "#             temp_sequence.append(y)\n",
    "#         one_exercise.append(temp_sequence)\n",
    "#         print(one_exercise[0][0],'------',temp_sequence[0])\n",
    "        \n",
    "\n",
    "#     # if max length of the exercise sequence was reached then pop it pop off\n",
    "#         if len(one_exercise) >= input_shape[1]:\n",
    "#             one_exercise.pop(0)\n",
    "\n",
    "#         all_array.append(one_exercise[:])\n",
    "        \n",
    "#         if frame_ctr >= frame_ctr_threshold:            \n",
    "#             frame_ctr = 0\n",
    "            \n",
    "#             prediction = np.array(one_exercise)\n",
    "#             prediction = prediction.astype(np.float32)\n",
    "#             prediction = prediction.reshape(1,input_shape[1],66)\n",
    "#             # current_prediction = model.predict(prediction)[0][0]\n",
    "#             # print(prediction.shape)\n",
    "#             model.set_tensor(input_details[0]['index'], prediction)\n",
    "#             model.invoke()\n",
    "#             output_data = model.get_tensor(output_details[0]['index'])\n",
    "#             # predicted_class = np.argmax(output_data)\n",
    "\n",
    "\n",
    "#             # print(output_data)\n",
    "#             if output_data >=.89:\n",
    "#                 exercise_ctr +=1\n",
    "#                 temp_checking.append(one_exercise)\n",
    "\n",
    "#                 # current_prediction = output_data\n",
    "#                 # if prev_prediction + prediction_buffer_threshold >= current_prediction and prev_prediction - prediction_buffer_threshold <= current_prediction:\n",
    "#                 #     None\n",
    "#                 # else:\n",
    "#                 #     exercise_ctr +=1\n",
    "\n",
    "#                 # prev_prediction = current_prediction                \n",
    "                \n",
    "\n",
    "#             # interpreter.reset_all_variables()\n",
    "        \n",
    "\n",
    "\n",
    "#         # interpreter.set_tensor(input_details[0]['index'], [prediction])\n",
    "#         # interpreter.invoke()\n",
    "#         # output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "#         # predicted_class = np.argmax(output_data)\n",
    "#         # print(\"Predicted Class:\", predicted_class)\n",
    "#         # print(\"Predicted Class:\", output_data)\n",
    "\n",
    "        \n",
    "#         # print(len(one_exercise))\n",
    "#         # print(prediction.shape)\n",
    "#         # \n",
    "#         # \n",
    "#         # if model.predict(prediction)[0][0]>=.8:\n",
    "#         #     print('correct')\n",
    "#     cv2.putText(frame, str(exercise_ctr), (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "#     cv2.putText(frame, str(output_data), (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         frame,\n",
    "#         results.pose_landmarks,\n",
    "#         mp_pose.POSE_CONNECTIONS,\n",
    "#         landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "#         connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "#     )\n",
    "\n",
    "#     cv2.imshow('MediaPipe Pose', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add increase the max length by certain amount(for now 10)\n",
    "# # soo it will always have more leeway when in the array\n",
    "# def padding(pre_processed_input,optional_maxLength=0,increase_maxlen =.4): \n",
    "#     padded_sequences = []\n",
    "#     if optional_maxLength != 0:\n",
    "#         max_length = optional_maxLength\n",
    "#     else:\n",
    "#         max_length = max(len(sequence) for sequence in pre_processed_input)\n",
    "#         max_length = max_length + (max_length *increase_maxlen )\n",
    "    \n",
    "#     for sequence in pre_processed_input:        \n",
    "#         padding_length = max_length - len(sequence)\n",
    "#         if padding_length >= 0:\n",
    "#             padded_sequence = np.pad(sequence, ((0, padding_length), (0, 0)), mode='constant')\n",
    "            \n",
    "#         else:\n",
    "#             padded_sequence = sequence[:max_length]\n",
    "#         padded_sequences.append(padded_sequence)\n",
    "#     padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "#     return padded_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do data augmentation that simulates the inferring process wherein it takes everyframes\n",
    "# therfore we do not only append infront but also behind  with several number \n",
    "# we insert and simulate tha sequence in somewhere in between the max length not only at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def streamlined_process(correct_execution,noise_data):\n",
    "#     base_data = txt_pre_process(correct_execution,1,True,4)\n",
    "#     base_data_noise = txt_pre_process(noise_data,0,True,4)\n",
    "\n",
    "#     data = base_data[0]\n",
    "#     data_noise = base_data_noise[0]\n",
    "\n",
    "#     print('')\n",
    "#     print('----------------------------correct data augmentation ----------------------------')\n",
    "#     aug = common_length_sequence(data)\n",
    "#     aug2 = apply_z_score(aug,1)\n",
    "#     aug3 = paddingV2(aug2)\n",
    "    \n",
    "#     print('')\n",
    "#     print('----------------------------data noise data augmentation ----------------------------')\n",
    "#     aug_noise = paddingV2(data_noise,len(aug3[0]))\n",
    "#     aug_noise = aug_noise[0:len(aug3)]\n",
    "\n",
    "\n",
    "#     aug3_label = np.ones(len(aug3))\n",
    "#     aug_noise_label = np.zeros(len(aug_noise))\n",
    "\n",
    "\n",
    "#     rand_batches=concatenate_randomize_batches(aug3,aug3_label,aug_noise,aug_noise_label)\n",
    "#     print('')\n",
    "#     print('----------------------------final num set of sequences ----------------------------')\n",
    "#     print('correct data ->',len(aug3))\n",
    "#     print('wrong data ->',len(aug_noise))\n",
    "#     print('combined and randomized data ->',rand_batches[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.1, random_state=42)\n",
    "\n",
    "#     # further splitting the dataset to have a set for simulation later on\n",
    "#     X_train, predict_simulate_input, y_train, predict_simulate_label = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(64, return_sequences=True, activation='relu', dropout=0.1, recurrent_dropout=0.1, input_shape=(len(aug3[0]), len(aug3[0][0]))))\n",
    "#     model.add(LSTM(128, return_sequences=True, activation='relu')) \n",
    "#     model.add(LSTM(64, return_sequences=False, activation='relu')) \n",
    "#     model.add(Dense(64, activation='relu')) \n",
    "#     model.add(Dense(32, activation='relu'))\n",
    "#     model.add(Dense(1,activation='sigmoid'))\n",
    "#     model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy', 'binary_accuracy'])\n",
    "\n",
    "#     model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "#     return(aug3,aug_noise)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlined_process('base(correct_execution).txt','base(wrong_execution).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(aug3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in aug3[0]:\n",
    "#     print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# index = 5\n",
    "\n",
    "# for x in range(len(testpop[index])):\n",
    "#     print(testpop[index][x][0],'---',aug3[index][x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_inputs = np.concatenate((testpop,testpop2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testpop = np.array(testpop)\n",
    "# testpop = testpop.reshape(-1,29,66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_data = txt_pre_process('correct_new.txt',1,True,4)\n",
    "# base_data_noise = txt_pre_process('wront_new.txt',0,True,4)\n",
    "\n",
    "# temp_correct_start=[]\n",
    "# temp_correct_wrong=[]\n",
    "\n",
    "# data = base_data[0]\n",
    "# data_noise = base_data_noise[0]\n",
    "\n",
    "# print('')\n",
    "# print('----------------------------correct data augmentation ----------------------------')\n",
    "# aug = common_length_sequence(data)\n",
    "# aug2 = apply_z_score(aug,1)\n",
    "# aug3 = paddingV2(aug2)\n",
    "# aug4 = populate_0_input(aug3,data_noise)\n",
    "# aug4 = np.array(aug4)\n",
    "# aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
    "# combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
    "# print('concat -> ', len(combined_inputs))\n",
    "\n",
    "\n",
    "# print('')\n",
    "# print('----------------------------data noise data augmentation ----------------------------')\n",
    "# aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
    "# aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
    "# aug_noise_data3 = aug_noise_data2[0:len(aug3)]\n",
    "# aug_noise_data4 = np.array(aug_noise_data3)\n",
    "\n",
    "# # for y in aug_noise_data4:\n",
    "# #   for x in y:\n",
    "# #     try:\n",
    "# #         print('-->',len(x))\n",
    "# #     except:\n",
    "# #        print(x)\n",
    "# #        break\n",
    "# #     break\n",
    "\n",
    "# # print(aug_noise_data4[0][4])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print('ppppppp ->',len(aug_noise_data4))\n",
    "# print('ppppppp ->',len(aug_noise_data4[0]))\n",
    "# print('ppppppp ->',len(aug_noise_data4[0][0]))\n",
    "# aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
    "\n",
    "\n",
    "# aug3_label = np.ones(len(aug3))\n",
    "# aug_noise_label = np.zeros(len(aug_noise_data1))\n",
    "\n",
    "\n",
    "\n",
    "# rand_batches=concatenate_randomize_batches(aug3,aug3_label,aug_noise_data5,aug_noise_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(testing_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.lite.Interpreter(model_path=\"D:\\CLARK\\Documents\\RNN\\RNN_proofOfConcept\\converted_model_1069default9.tflite\")\n",
    "# base_data = txt_pre_process('base1234.txt',1,False,4)\n",
    "\n",
    "# # 15\n",
    "\n",
    "\n",
    "# model.allocate_tensors()\n",
    "# input_details = model.get_input_details()\n",
    "# output_details = model.get_output_details()\n",
    "# input_shape = input_details[0]['shape']\n",
    "\n",
    "\n",
    "# data = base_data[0]\n",
    "\n",
    "\n",
    "# w_aug3=[]\n",
    "# for x in data:\n",
    "#     if (input_shape[1] - len(x))>0:\n",
    "#         w_aug3.append(np.pad(x, ((0, input_shape[1] - len(x)), (0, 0)), mode='constant'))\n",
    "#     else:\n",
    "#         print('----')\n",
    "#         continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ctr = 0\n",
    "# for x in w_aug3:      \n",
    "#     prediction = np.array(x)\n",
    "#     prediction = prediction.astype(np.float32)\n",
    "#     prediction = prediction.reshape(-1,input_shape[1],66)\n",
    "\n",
    "#     model.set_tensor(input_details[0]['index'], prediction)\n",
    "#     model.invoke()\n",
    "#     output_data = model.get_tensor(output_details[0]['index'])\n",
    "#     print(output_data[0][0])\n",
    "#     if output_data[0][0] >= .99:\n",
    "#         ctr = ctr + 1\n",
    "#     else:\n",
    "#         None\n",
    "#         # print(output_data[0][0])\n",
    "# print(ctr)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_base_data = txt_pre_process('wront_new.txt',1,True,4)\n",
    "# w_base_data = w_base_data[0]\n",
    "# w_aug = common_length_sequence(w_base_data)\n",
    "# w_aug2 = apply_z_score(w_aug,1)\n",
    "# w_aug3 = paddingV2(w_aug2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = []\n",
    "# temp_compile = []\n",
    "# len_seq = 29\n",
    "# for x in range(len_seq):\n",
    "#     temp.append(0)\n",
    "\n",
    "# for x in range(len(all_array[0:120])):\n",
    "#         temp.pop(0)\n",
    "#         temp.append(all_array[x])\n",
    "\n",
    "    \n",
    "#     temp_compile.append(temp)\n",
    "    \n",
    "\n",
    "# print(len(temp_compile[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for u in range(20):\n",
    "#     w_temp = []\n",
    "#     for x in range(len(aug3[1])):  \n",
    "#         w_temp.append(w_aug3[u][rand.randint(1,10)])\n",
    "\n",
    "\n",
    "\n",
    "#     prediction = np.array(w_temp)\n",
    "#     prediction = prediction.astype(np.float32)\n",
    "#     prediction = prediction.reshape(1,input_shape[1],66)\n",
    "\n",
    "#     model.set_tensor(input_details[0]['index'], prediction)\n",
    "#     model.invoke()\n",
    "#     output_data = model.get_tensor(output_details[0]['index'])\n",
    "#     print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_temp = []\n",
    "# for x in range(len(aug3[1])):  \n",
    "#     w_temp.append(w_aug3[0][rand.randint(1,10)])\n",
    "\n",
    "\n",
    "\n",
    "# prediction = np.array(w_temp)\n",
    "# prediction = prediction.astype(np.float32)\n",
    "# prediction = prediction.reshape(1,input_shape[1],66)\n",
    "\n",
    "# model.set_tensor(input_details[0]['index'], prediction)\n",
    "# model.invoke()\n",
    "# output_data = model.get_tensor(output_details[0]['index'])\n",
    "# print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def populate_0_input(correct_data,noise_data):\n",
    "#     index = 10\n",
    "#     temp = []\n",
    "#     temp_compilation = []\n",
    "#     ctr = 0\n",
    "#     rand_modifier =0\n",
    "    \n",
    "#     for set_sequence in correct_data:\n",
    "#         rand_modifier = rand.randint(0,len(noise_data))\n",
    "\n",
    "#         for x in range(len(aug3[1])):\n",
    "#             ctr = ctr + 1 \n",
    "\n",
    "#             if aug3[index][x][0] == 0:\n",
    "#                 temp.append(noise_data[rand_modifier][rand.randint(1,len(noise_data[rand_modifier]))])\n",
    "#             else:\n",
    "#                 temp.append(set_sequence[x])\n",
    "\n",
    "#             temp_compilation.append(temp)\n",
    "#             temp =[]\n",
    "\n",
    "\n",
    "#     # temp = np.array(temp)\n",
    "#     # temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# prediction = np.array(aug3[x])\n",
    "# prediction = prediction.astype(np.float32)\n",
    "# prediction = prediction.reshape(-1,input_shape[1],66)\n",
    "\n",
    "# model.set_tensor(input_details[0]['index'], prediction)\n",
    "# model.invoke()\n",
    "# output_data = model.get_tensor(output_details[0]['index'])\n",
    "# if output_data[0][0] >= .99:\n",
    "#     ctr = ctr + 1\n",
    "# else:\n",
    "#     None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is inferencing with checkpoint\n",
    "\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# from IPython.display import clear_output\n",
    "# temp_testing_int_1 = []\n",
    "# temp_testing_int_2 = []\n",
    "\n",
    "# mp_pose = mp.solutions.pose\n",
    "# pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # cap = cv2.VideoCapture(\"vid4.mp4\")\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# # PARAMETERS:\n",
    "# # ================================================================================================================================\n",
    "# # timer to ready before posing/getting ready for the excercise\n",
    "# set_timer_start = 50\n",
    "\n",
    "# # ================================================================================================================================\n",
    "# # range of how much change will it be considered a movement\n",
    "# # lower -> small movement and jittery flaw of mediapipe will be detected\n",
    "# # higher -> it wont detect immediately important frames\n",
    "# # 0-1\n",
    "# change_range = .03\n",
    "\n",
    "# # ================================================================================================================================\n",
    "# # a counter for the amount of time for the wait time before performing the exercise again\n",
    "# between_exercise_ctr = 10\n",
    "\n",
    "\n",
    "\n",
    "# isInitiated = False\n",
    "# exercise_exectuted_ctr=0\n",
    "# prev_x = 0\n",
    "# prev_y = 0\n",
    "# no_movement_ctr = 0\n",
    "# movement_boolean = False\n",
    "# timer = 0\n",
    "# no_movement_threshold = 32\n",
    "# for_prediction = []\n",
    "# prev_array = []\n",
    "\n",
    "\n",
    "# model = tf.lite.Interpreter(model_path=\"D:\\CLARK\\Documents\\RNN\\RNN_proofOfConcept\\converted_model_whole_model4627(loss_0.029)(acc_1.0).tflite\")\n",
    "\n",
    "\n",
    "\n",
    "# model.allocate_tensors()\n",
    "# input_details = model.get_input_details()\n",
    "# output_details = model.get_output_details()\n",
    "# input_shape = input_details[0]['shape']\n",
    "\n",
    "# for x in range(33):\n",
    "#     prev_array.append([0, 0])\n",
    "\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = pose.process(image_rgb)\n",
    "\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     timer += 1 \n",
    "#     landmark_all_present = False\n",
    "\n",
    "#     if timer == set_timer_start:\n",
    "#         file = open('base1234.txt', 'w')\n",
    "#         if isInitiated==False:\n",
    "#             file.write(\"START\")\n",
    "#             file.write('\\n')\n",
    "#         isInitiated = True\n",
    "\n",
    "#     if results.pose_landmarks:\n",
    "#         ctr_check=0\n",
    "\n",
    "#         for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "#             if idx == 32:\n",
    "#                 landmark_all_present = \"Landmarks -> ALL PRESENT\"\n",
    "#             else:\n",
    "#                 landmark_all_present = \"Landmarks -> SOME ARE MISSING\"\n",
    "\n",
    "#             x = landmark.x\n",
    "#             y = landmark.y\n",
    "\n",
    "# #checks if no movement is detected-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "#             if x >= (prev_array[idx][0] - change_range) and y <= (prev_array[idx][1] + change_range) and isInitiated:   \n",
    "#                 # print(idx,\")\",x,\"---\",prev_array[idx][0],'---> NO MOVEMENT')\n",
    "#                 ctr_check+=1          \n",
    "                \n",
    "#             else:\n",
    "#                 # print(idx,\")\",x,\"---\",prev_array[idx][0],'--->YES MOVEMENT',)\n",
    "#                 movement_boolean = True\n",
    "#                 no_movement_ctr = 0\n",
    "\n",
    "#             if ctr_check == no_movement_threshold:\n",
    "#                 no_movement_ctr += 1 \n",
    "\n",
    "#             if no_movement_ctr>=25:\n",
    "#                 movement_boolean=False\n",
    "#             prev_array[idx] = [x, y]\n",
    "#         # print('counter->',no_movement_ctr)\n",
    "#         # print(movement_boolean)\n",
    "\n",
    "# #if movement is detected then it will record the coordinates into a text file-----------------------------------------------------------------------------------------------------\n",
    "#         if movement_boolean:\n",
    "#             text = \"MOVEMENT\"      \n",
    "#             if timer >= set_timer_start and file is not None:\n",
    "#                 for landmark in prev_array:\n",
    "#                     # file.write(\"<\")\n",
    "#                     file.write(\"|\")\n",
    "#                     file.write(str(landmark[0]))\n",
    "#                     file.write(\"|\")\n",
    "#                     file.write(str(landmark[1]))\n",
    "#                     temp_testing_int_1.append(landmark[0])\n",
    "#                     temp_testing_int_1.append(landmark[1])\n",
    "#                     # file.write(\">\")\n",
    "#                     # file.write(\"|\")\n",
    "#                 file.write('\\n')\n",
    "#                 temp_testing_int_2.append(temp_testing_int_1)\n",
    "#                 temp_testing_int_1=[]\n",
    "\n",
    "#         if no_movement_ctr >= 5:    \n",
    "#             movement_boolean = False\n",
    "#             text = \"NO MOVEMENT!\"\n",
    "\n",
    "#         if no_movement_ctr == between_exercise_ctr:\n",
    "#             print(len(temp_testing_int_2) ,'>',input_shape[1])\n",
    "#             while len(temp_testing_int_2) > input_shape[1]:\n",
    "#                 temp_testing_int_2.pop(0)\n",
    "#                 # print('poping')\n",
    "\n",
    "#             if len(temp_testing_int_2) <= input_shape[1]:\n",
    "#                 # print('padding')\n",
    "#                 temp_testing_int_2 = np.pad(temp_testing_int_2, ((0, input_shape[1] - len(temp_testing_int_2)), (0, 0)), mode='constant')\n",
    "\n",
    "#             prediction = np.array(temp_testing_int_2)\n",
    "#             print('===========',len(temp_testing_int_2))\n",
    "#             print('===========',len(temp_testing_int_2[0]))\n",
    "#             prediction = prediction.astype(np.float32)\n",
    "#             prediction = prediction.reshape(1,input_shape[1],66)\n",
    "#             # current_prediction = model.predict(prediction)[0][0]\n",
    "#             # print(prediction.shape)\n",
    "#             model.set_tensor(input_details[0]['index'], prediction)\n",
    "#             model.invoke()\n",
    "#             output_data = model.get_tensor(output_details[0]['index'])\n",
    "#             print('--->',output_data)\n",
    "#             clear_output(wait=True)\n",
    "#             temp_testing_int_2=[]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "#             exercise_exectuted_ctr+=1\n",
    "#             file.write(\"END\")  \n",
    "#             file.write('\\n')\n",
    "#             file.write('\\n')\n",
    "#             file.write(\"START\")\n",
    "#             file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#         if timer >= set_timer_start and no_movement_ctr >=between_exercise_ctr:\n",
    "#             cv2.putText(frame, \"PERFORM THE EXERCISE\", (180, 460), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n",
    "# # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# #Outputing important informations-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#         cv2.putText(frame, str(movement_boolean), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "#         cv2.putText(frame, f\"no_movement_ctr-> {no_movement_ctr}\", (20, 37), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "#         cv2.putText(frame, str(landmark_all_present), (20, 54), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "#         cv2.putText(frame, f\"timer-> {timer}\", (20, 73), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "#         cv2.putText(frame, f\"exercise exectuted-> {exercise_exectuted_ctr}\", (20, 92), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "# # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#         if timer < set_timer_start:\n",
    "#             cv2.putText(frame, \"PREPARE YOUR STARTING POSE\", (180, 460), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n",
    "            \n",
    "\n",
    "#         mp_drawing.draw_landmarks(\n",
    "#             frame,\n",
    "#             results.pose_landmarks,\n",
    "#             mp_pose.POSE_CONNECTIONS,\n",
    "#             landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "#             connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "#         )\n",
    "\n",
    "#     cv2.imshow('MediaPipe Pose', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# if file is not None:\n",
    "#     file.close()\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is inferencing with checkpoint\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "temp_testing_int_1 = []\n",
    "temp_testing_int_2 = []\n",
    "\n",
    "\n",
    "wrong_activator_line = []\n",
    "\n",
    "for x in range(14):\n",
    "    wrong_activator_line.append(0)\n",
    "\n",
    "hand = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---(for storing of sequences)---------------------------------------\n",
    "left_upper_arm_temp = []\n",
    "left_lower_arm_temp = []\n",
    "left_hand_temp = []\n",
    "\n",
    "right_upper_arm_temp = []\n",
    "right_lower_arm_temp= []\n",
    "right_hand_temp = []\n",
    "\n",
    "left_upper_leg_temp = []\n",
    "left_lower_leg_temp = []\n",
    "left_feet_temp = []\n",
    "\n",
    "right_upper_leg_temp = []\n",
    "right_lower_leg_temp= []\n",
    "right_feet_temp = []\n",
    "\n",
    "body_temp = []\n",
    "head_temp= []\n",
    "# --------------------------------------------------------------------------------------\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "temp_determiner = 0\n",
    "\n",
    "# cap = cv2.VideoCapture(\"vid4.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# PARAMETERS:\n",
    "# ================================================================================================================================\n",
    "# timer to ready before posing/getting ready for the excercise\n",
    "set_timer_start = 50\n",
    "\n",
    "# ================================================================================================================================\n",
    "# range of how much change will it be considered a movement\n",
    "# lower -> small movement and jittery flaw of mediapipe will be detected\n",
    "# higher -> it wont detect immediately important frames\n",
    "# 0-1\n",
    "change_range = .03\n",
    "\n",
    "# ================================================================================================================================\n",
    "# a counter for the amount of time for the wait time before performing the exercise again\n",
    "between_exercise_ctr = 15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "isInitiated = False\n",
    "exercise_exectuted_ctr=0\n",
    "prev_x = 0\n",
    "prev_y = 0\n",
    "no_movement_ctr = 0\n",
    "movement_boolean = False\n",
    "timer = 0\n",
    "no_movement_threshold = 32\n",
    "for_prediction = []\n",
    "prev_array = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\individual_models_1\\otestingtesting(loss_0.063)(acc_0.982).tflite\")\n",
    "\n",
    "\n",
    "model_left_upper_arm = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\updated_model\\\\converted_model_left_upper_arm6093(loss_0.049)(acc_0.984).tflite\")\n",
    "model_left_lower_arm = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\updated_model\\\\converted_model_left_lower_arm6093(loss_0.137)(acc_0.963).tflite\")\n",
    "# model_left_hand = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\individual_models_1\\\\converted_model_5016left_hand.tflite\")\n",
    "\n",
    "\n",
    "model_right_upper_arm = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\updated_model\\\\converted_model_right_upper_arm6093(loss_0.105)(acc_0.952).tflite\")\n",
    "model_right_lower_arm = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\updated_model\\\\converted_model_right_lower_arm6093(loss_0.199)(acc_0.957).tflite\")\n",
    "# model_right_hand = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\individual_models_1\\\\converted_model_1334right_hand.tflite\")\n",
    "\n",
    "\n",
    "model_left_upper_leg = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\updated_model\\\\converted_model_left_upper_leg6093(loss_0.083)(acc_0.973).tflite\")\n",
    "model_left_lower_leg = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\updated_model\\\\converted_model_left_lower_leg6093(loss_0.046)(acc_0.989).tflite\")\n",
    "# model_left_feet = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\individual_models_1\\\\converted_model_5286left_feet.tflite\")\n",
    "\n",
    "\n",
    "model_right_upper_leg = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\updated_model\\\\converted_model_right_upper_leg6093(loss_0.239)(acc_0.915).tflite\")\n",
    "model_right_lower_leg = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\updated_model\\\\converted_model_right_lower_leg6093(loss_0.57)(acc_0.84).tflite\")\n",
    "# model_right_feet = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\individual_models_1\\\\converted_model_6732right_feet.tflite\")\n",
    "\n",
    "\n",
    "# model_body = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\converted_model_9473left_upper_arm.tflite\")\n",
    "# model_head = tf.lite.Interpreter(model_path=\"D:\\\\CLARK\\\\Documents\\\\RNN\\\\RNN_proofOfConcept\\\\individual_models_1\\\\converted_model_7222head.tflite\")\n",
    "\n",
    "# base_data = txt_pre_process('base1234.txt',1,False,4)\n",
    "\n",
    "# -----------model-------------------------\n",
    "model.allocate_tensors()\n",
    "input_details = model.get_input_details()\n",
    "output_details = model.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================================================================\n",
    "# -----------left_upper_arm-------------------------\n",
    "model_left_upper_arm.allocate_tensors()\n",
    "input_details_left_upper_arm = model_left_upper_arm.get_input_details()\n",
    "output_details_left_upper_arm = model_left_upper_arm.get_output_details()\n",
    "input_shape_left_upper_arm = input_details_left_upper_arm[0]['shape']\n",
    "\n",
    "# -----------left_lower_arm-------------------------\n",
    "model_left_lower_arm.allocate_tensors()\n",
    "input_details_left_lower_arm = model_left_lower_arm.get_input_details()\n",
    "output_details_left_lower_arm = model_left_lower_arm.get_output_details()\n",
    "input_shape_left_lower_arm = input_details_left_lower_arm[0]['shape']\n",
    "\n",
    "# -----------left_hand-------------------------\n",
    "# model_left_hand.allocate_tensors()\n",
    "# input_details_left_hand = model_left_hand.get_input_details()\n",
    "# output_details_left_hand = model_left_hand.get_output_details()\n",
    "# input_shape_left_hand = input_details_left_hand[0]['shape']\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================================================================\n",
    "# -----------right_upper_arm-------------------------\n",
    "model_right_upper_arm.allocate_tensors()\n",
    "input_details_right_upper_arm = model_right_upper_arm.get_input_details()\n",
    "output_details_right_upper_arm = model_right_upper_arm.get_output_details()\n",
    "input_shape_right_upper_arm = input_details_right_upper_arm[0]['shape']\n",
    "\n",
    "# -----------right_lower_arm-------------------------\n",
    "model_right_lower_arm.allocate_tensors()\n",
    "input_details_right_lower_arm = model_right_lower_arm.get_input_details()\n",
    "output_details_right_lower_arm = model_right_lower_arm.get_output_details()\n",
    "input_shape_right_lower_arm = input_details_right_lower_arm[0]['shape']\n",
    "\n",
    "# -----------right_hand-------------------------\n",
    "# model_right_hand.allocate_tensors()\n",
    "# input_details_right_hand = model_right_hand.get_input_details()\n",
    "# output_details_right_hand = model_right_hand.get_output_details()\n",
    "# input_shape_right_hand = input_details_right_hand[0]['shape']\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "# ===========================================================================================\n",
    "# -----------left_upper_leg-------------------------\n",
    "model_left_upper_leg.allocate_tensors()\n",
    "input_details_left_upper_leg = model_left_upper_leg.get_input_details()\n",
    "output_details_left_upper_leg = model_left_upper_leg.get_output_details()\n",
    "input_shape_left_upper_leg = input_details_left_upper_leg[0]['shape']\n",
    "\n",
    "# -----------left_lower_leg-------------------------\n",
    "model_left_lower_leg.allocate_tensors()\n",
    "input_details_left_lower_leg = model_left_lower_leg.get_input_details()\n",
    "output_details_left_lower_leg = model_left_lower_leg.get_output_details()\n",
    "input_shape_left_lower_leg = input_details_left_lower_leg[0]['shape']\n",
    "\n",
    "# -----------left_feet-------------------------\n",
    "# model_left_feet.allocate_tensors()\n",
    "# input_details_left_feet = model_left_feet.get_input_details()\n",
    "# output_details_left_feet = model_left_feet.get_output_details()\n",
    "# input_shape_left_feet = input_details_left_feet[0]['shape']\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "# ===========================================================================================\n",
    "# -----------right_upper_leg-------------------------\n",
    "model_right_upper_leg.allocate_tensors()\n",
    "input_details_right_upper_leg = model_right_upper_leg.get_input_details()\n",
    "output_details_right_upper_leg = model_right_upper_leg.get_output_details()\n",
    "input_shape_right_upper_leg = input_details_right_upper_leg[0]['shape']\n",
    "\n",
    "# -----------right_lower_leg-------------------------\n",
    "model_right_lower_leg.allocate_tensors()\n",
    "input_details_right_lower_leg = model_right_lower_leg.get_input_details()\n",
    "output_details_right_lower_leg = model_right_lower_leg.get_output_details()\n",
    "input_shape_right_lower_leg = input_details_right_lower_leg[0]['shape']\n",
    "\n",
    "# -----------right_feet-------------------------\n",
    "# model_right_feet.allocate_tensors()\n",
    "# input_details_right_feet = model_right_feet.get_input_details()\n",
    "# output_details_right_feet = model_right_feet.get_output_details()\n",
    "# input_shape_right_feet = input_details_right_feet[0]['shape']\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "# ===========================================================================================\n",
    "# -----------body_model-------------------------\n",
    "# model_body.allocate_tensors()\n",
    "# input_details_body = model_body.get_input_details()\n",
    "# output_details_body = model_body.get_output_details()\n",
    "# input_shape_body = input_details_body[0]['shape']\n",
    "\n",
    "# -----------head_model-------------------------\n",
    "# model_head.allocate_tensors()\n",
    "# input_details_head = model_head.get_input_details()\n",
    "# output_details_head = model_head.get_output_details()\n",
    "# input_shape_head = input_details_head[0]['shape']\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "left_upper_arm_compare = 0\n",
    "left_lower_arm_compare = 0\n",
    "left_hand_temp_compare = 0\n",
    "right_upper_arm_compare = 0\n",
    "right_lower_arm_compare = 0\n",
    "right_hand_compare = 0\n",
    "left_upper_leg_compare = 0\n",
    "left_lower_leg_compare = 0\n",
    "left_feet_compare = 0\n",
    "right_upper_leg_compare = 0\n",
    "right_lower_leg_compare = 0\n",
    "right_feet_compare = 0\n",
    "body_temp_compare = 0\n",
    "head_temp_compare = 0\n",
    "\n",
    "correct_threshold = 0\n",
    "\n",
    "for x in range(33):\n",
    "    prev_array.append([0, 0])\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    timer += 1 \n",
    "    landmark_all_present = False\n",
    "\n",
    "    if timer == set_timer_start:\n",
    "        isInitiated = True\n",
    "\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        ctr_check=0\n",
    "\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx == 32:\n",
    "                landmark_all_present = \"Landmarks -> ALL PRESENT\"\n",
    "            else:\n",
    "                landmark_all_present = \"Landmarks -> SOME ARE MISSING\"\n",
    "\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "\n",
    "#checks if no movement is detected-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "            if x >= (prev_array[idx][0] - change_range) and y <= (prev_array[idx][1] + change_range) and isInitiated:   \n",
    "                ctr_check+=1          \n",
    "                \n",
    "            else:\n",
    "                movement_boolean = True\n",
    "                no_movement_ctr = 0\n",
    "\n",
    "            if ctr_check == no_movement_threshold:\n",
    "                no_movement_ctr += 1 \n",
    "\n",
    "            if no_movement_ctr>=25:\n",
    "                movement_boolean=False\n",
    "\n",
    "            prev_array[idx] = [x, y]\n",
    "\n",
    "\n",
    "#if movement is detected then it will record the coordinates into a text file-----------------------------------------------------------------------------------------------------\n",
    "        if movement_boolean:    \n",
    "\n",
    "            # temp_testing_int_2=[]\n",
    "            # hand=[]\n",
    "\n",
    "            # left_upper_arm_temp=[]\n",
    "            # left_lower_arm_temp=[]\n",
    "            # left_hand_temp=[]\n",
    "\n",
    "            # right_upper_arm_temp=[]\n",
    "            # right_lower_arm_temp=[]\n",
    "            # right_hand_temp=[]\n",
    "\n",
    "            # left_upper_leg_temp=[]\n",
    "            # left_lower_leg_temp=[]\n",
    "            # left_feet_temp=[]\n",
    "\n",
    "            # right_upper_leg_temp=[]\n",
    "            # right_lower_leg_temp=[]\n",
    "            # right_feet_temp=[]\n",
    "\n",
    "            # body_temp=[]\n",
    "            # head_temp=[] \n",
    "\n",
    "            if timer >= set_timer_start :\n",
    "                for landmark in prev_array:\n",
    "                    temp_testing_int_1.append(landmark[0])\n",
    "                    temp_testing_int_1.append(landmark[1])\n",
    "                temp_testing_int_2.append(temp_testing_int_1)\n",
    "                # hand.append([temp_testing_int_1[14],temp_testing_int_1[15],temp_testing_int_1[16],temp_testing_int_1[17],temp_testing_int_1[18],temp_testing_int_1[19],temp_testing_int_1[20],temp_testing_int_1[21]])\n",
    "                # hand.append([temp_testing_int_1[22],temp_testing_int_1[23],temp_testing_int_1[26],temp_testing_int_1[27]])\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "                left_upper_arm_temp.append([temp_testing_int_1[22],temp_testing_int_1[23],temp_testing_int_1[26],temp_testing_int_1[27]])\n",
    "                left_lower_arm_temp.append([temp_testing_int_1[26],temp_testing_int_1[27],temp_testing_int_1[30],temp_testing_int_1[31]])\n",
    "                # left_hand_temp.append([temp_testing_int_1[30],temp_testing_int_1[31],temp_testing_int_1[34],temp_testing_int_1[35],temp_testing_int_1[38],temp_testing_int_1[39],temp_testing_int_1[42],temp_testing_int_1[43]])\n",
    "\n",
    "                right_upper_arm_temp.append([temp_testing_int_1[24],temp_testing_int_1[25],temp_testing_int_1[28],temp_testing_int_1[29]])\n",
    "                right_lower_arm_temp.append([temp_testing_int_1[28],temp_testing_int_1[29],temp_testing_int_1[32],temp_testing_int_1[33]])\n",
    "                # right_hand_temp.append([temp_testing_int_1[32],temp_testing_int_1[33],temp_testing_int_1[36],temp_testing_int_1[37],temp_testing_int_1[40],temp_testing_int_1[41],temp_testing_int_1[44],temp_testing_int_1[45]])\n",
    "\n",
    "                left_upper_leg_temp.append([temp_testing_int_1[46],temp_testing_int_1[47],temp_testing_int_1[50],temp_testing_int_1[51]])\n",
    "                left_lower_leg_temp.append([temp_testing_int_1[50],temp_testing_int_1[51],temp_testing_int_1[54],temp_testing_int_1[55]])\n",
    "                # left_feet_temp.append([temp_testing_int_1[54],temp_testing_int_1[55],temp_testing_int_1[58],temp_testing_int_1[59],temp_testing_int_1[62],temp_testing_int_1[63]])\n",
    "\n",
    "                right_upper_leg_temp.append([temp_testing_int_1[48],temp_testing_int_1[49],temp_testing_int_1[52],temp_testing_int_1[53]])\n",
    "                right_lower_leg_temp.append([temp_testing_int_1[52],temp_testing_int_1[53],temp_testing_int_1[56],temp_testing_int_1[57]])\n",
    "                # right_feet_temp.append([temp_testing_int_1[56],temp_testing_int_1[57],temp_testing_int_1[60],temp_testing_int_1[61],temp_testing_int_1[64],temp_testing_int_1[65]])\n",
    "\n",
    "                # body_temp.append([temp_testing_int_1[22],temp_testing_int_1[23],temp_testing_int_1[24],temp_testing_int_1[25],temp_testing_int_1[46],temp_testing_int_1[47],temp_testing_int_1[48],temp_testing_int_1[49]])\n",
    "                # head_temp.append([temp_testing_int_1[14],temp_testing_int_1[15],temp_testing_int_1[16],temp_testing_int_1[17],temp_testing_int_1[18],temp_testing_int_1[19],temp_testing_int_1[20],temp_testing_int_1[21]])\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                temp_testing_int_1=[]\n",
    "\n",
    "        if no_movement_ctr >= 5:    \n",
    "            movement_boolean = False\n",
    "            text = \"NO MOVEMENT!\"\n",
    "\n",
    "        if no_movement_ctr == between_exercise_ctr:\n",
    "            while len(temp_testing_int_2) > input_shape[1]:\n",
    "                temp_testing_int_2.pop(0)\n",
    "                # hand.pop(0)\n",
    "\n",
    "                left_upper_arm_temp.pop(0)\n",
    "                left_lower_arm_temp.pop(0)\n",
    "                # left_hand_temp.pop(0)\n",
    "\n",
    "                right_upper_arm_temp.pop(0)\n",
    "                right_lower_arm_temp.pop(0)\n",
    "                # right_hand_temp.pop(0)\n",
    "\n",
    "                left_upper_leg_temp.pop(0)\n",
    "                left_lower_leg_temp.pop(0)\n",
    "                # left_feet_temp.pop(0)\n",
    "\n",
    "                right_upper_leg_temp.pop(0)\n",
    "                right_lower_leg_temp.pop(0)\n",
    "                # right_feet_temp.pop(0)\n",
    "\n",
    "                # body_temp.pop(0)\n",
    "                # head_temp.pop(0)\n",
    "\n",
    "                \n",
    "            temp_determiner = len(left_upper_arm_temp)-1\n",
    "\n",
    "            if len(temp_testing_int_2) <= input_shape[1]:\n",
    "                print('input_shape->',input_shape[1])\n",
    "                print('input_shape->',temp_testing_int_2)\n",
    "                temp_testing_int_2 = np.pad(temp_testing_int_2, ((0, input_shape[1] - len(temp_testing_int_2)), (0, 0)), mode='constant')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "                temp_determiner = len(left_upper_arm_temp)-1\n",
    "\n",
    "                print('test1->',input_shape_left_upper_arm[1])\n",
    "                print('test1->',len(left_upper_arm_temp))\n",
    "\n",
    "            if len(left_upper_arm_temp) <= input_shape_left_upper_arm[1]:\n",
    "                left_upper_arm_temp = np.pad(left_upper_arm_temp, ((0, input_shape_left_upper_arm[1] - len(left_upper_arm_temp)), (0, 0)), mode='constant')\n",
    "                left_lower_arm_temp = np.pad(left_lower_arm_temp, ((0, input_shape_left_lower_arm[1] - len(left_lower_arm_temp)), (0, 0)), mode='constant')\n",
    "                # left_hand_temp = np.pad(left_hand_temp, ((0, input_shape_left_hand[1] - len(left_hand_temp)), (0, 0)), mode='constant')\n",
    "\n",
    "                right_upper_arm_temp = np.pad(right_upper_arm_temp, ((0, input_shape_right_upper_arm[1] - len(right_upper_arm_temp)), (0, 0)), mode='constant')\n",
    "                right_lower_arm_temp = np.pad(right_lower_arm_temp, ((0, input_shape_right_lower_arm[1] - len(right_lower_arm_temp)), (0, 0)), mode='constant')\n",
    "                # right_hand_temp = np.pad(right_hand_temp, ((0, input_shape_right_hand[1] - len(right_hand_temp)), (0, 0)), mode='constant')\n",
    "\n",
    "                left_upper_leg_temp = np.pad(left_upper_leg_temp, ((0, input_shape_left_upper_leg[1] - len(left_upper_leg_temp)), (0, 0)), mode='constant')\n",
    "                left_lower_leg_temp = np.pad(left_lower_leg_temp, ((0, input_shape_left_lower_leg[1] - len(left_lower_leg_temp)), (0, 0)), mode='constant')\n",
    "                # left_feet_temp = np.pad(left_feet_temp, ((0, input_shape_left_feet[1] - len(left_feet_temp)), (0, 0)), mode='constant')\n",
    "\n",
    "                right_upper_leg_temp = np.pad(right_upper_leg_temp, ((0, input_shape_right_upper_leg[1] - len(right_upper_leg_temp)), (0, 0)), mode='constant')\n",
    "                right_lower_leg_temp = np.pad(right_lower_leg_temp, ((0, input_shape_right_lower_leg[1] - len(right_lower_leg_temp)), (0, 0)), mode='constant')\n",
    "                # right_feet_temp = np.pad(right_feet_temp, ((0, input_shape_right_feet[1] - len(right_feet_temp)), (0, 0)), mode='constant')\n",
    "\n",
    "                # body_temp = np.pad(body_temp, ((0, input_shape_body[1] - len(body_temp)), (0, 0)), mode='constant')\n",
    "                # head_temp = np.pad(head_temp, ((0, input_shape_head[1] - len(head_temp)), (0, 0)), mode='constant')\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            prediction = np.array(temp_testing_int_2)\n",
    "            prediction = prediction.astype(np.float32)\n",
    "            prediction = prediction.reshape(1,input_shape[1],input_shape[2])\n",
    "            model.set_tensor(input_details[0]['index'], prediction)\n",
    "            model.invoke()            \n",
    "            output_data = model.get_tensor(output_details[0]['index'])\n",
    "            print('whole --->',output_data)\n",
    "          \n",
    "\n",
    "            # model_inference(hand,model_hand,input_shape_hand,input_details_hand,output_details_hand,'left_upper_arm')\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            correct_threshold = .85           \n",
    "            left_upper_arm_compare = model_inference(left_upper_arm_temp,model_left_upper_arm,input_shape_left_upper_arm,input_details_left_upper_arm,output_details_left_upper_arm,'left_upper_arm') \n",
    "            left_lower_arm_compare =  model_inference(left_lower_arm_temp,model_left_lower_arm,input_shape_left_lower_arm,input_details_left_lower_arm,output_details_left_lower_arm,'left_lower_arm')\n",
    "            # left_hand_temp_compare = model_inference(left_hand_temp,model_left_hand,input_shape_left_hand,input_details_left_hand,output_details_left_hand,'left_hand')\n",
    "\n",
    "\n",
    "            right_upper_arm_compare = model_inference(right_upper_arm_temp,model_right_upper_arm,input_shape_right_upper_arm,input_details_right_upper_arm,output_details_right_upper_arm,'right_upper_arm')                     \n",
    "            right_lower_arm_compare = model_inference(right_lower_arm_temp,model_right_lower_arm,input_shape_right_lower_arm,input_details_right_lower_arm,output_details_right_lower_arm,'right_lower_arm')\n",
    "            # right_hand_compare =  model_inference(right_hand_temp,model_right_hand,input_shape_right_hand,input_details_right_hand,output_details_right_hand,'right_hand')>=correct_threshold:\n",
    "\n",
    "            \n",
    "\n",
    "            left_upper_leg_compare =  model_inference(left_upper_leg_temp,model_left_upper_leg,input_shape_left_upper_leg,input_details_left_upper_leg,output_details_left_upper_leg,'left_upper_leg')\n",
    "            left_lower_leg_compare =  model_inference(left_lower_leg_temp,model_left_lower_leg,input_shape_left_lower_leg,input_details_left_lower_leg,output_details_left_lower_leg,'left_lower_leg')\n",
    "            # left_feet_compare = model_inference(left_feet_temp,model_left_feet,input_shape_left_feet,input_details_left_feet,output_details_left_feet,'left_feet')>=correct_threshold:\n",
    "\n",
    "\n",
    "            right_upper_leg_compare =  model_inference(right_upper_leg_temp,model_right_upper_leg,input_shape_right_upper_leg,input_details_right_upper_leg,output_details_right_upper_leg,'right_upper_leg')\n",
    "            right_lower_leg_compare =  model_inference(right_lower_leg_temp,model_right_lower_leg,input_shape_right_lower_leg,input_details_right_lower_leg,output_details_right_lower_leg,'right_lower_leg')\n",
    "            # right_feet_compare =  model_inference(right_feet_temp,model_right_feet,input_shape_right_feet,input_details_right_feet,output_details_right_feet,'right_feet')>=correct_threshold:\n",
    "\n",
    "\n",
    "\n",
    "            # body_temp_compare =  model_inference(body_temp,model_body,input_shape_body,input_details_body,output_details_body,'body_model'):           \n",
    "            # head_temp_compare =  model_inference(head_temp,model_head,input_shape_head,input_details_head,output_details_head,'head_model'):\n",
    "\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            temp_testing_int_2=[]\n",
    "            hand=[]\n",
    "\n",
    "            left_upper_arm_temp=[]\n",
    "            left_lower_arm_temp=[]\n",
    "            left_hand_temp=[]\n",
    "\n",
    "            right_upper_arm_temp=[]\n",
    "            right_lower_arm_temp=[]\n",
    "            right_hand_temp=[]\n",
    "\n",
    "            left_upper_leg_temp=[]\n",
    "            left_lower_leg_temp=[]\n",
    "            left_feet_temp=[]\n",
    "\n",
    "            right_upper_leg_temp=[]\n",
    "            right_lower_leg_temp=[]\n",
    "            right_feet_temp=[]\n",
    "\n",
    "            body_temp=[]\n",
    "            head_temp=[]             \n",
    "            exercise_exectuted_ctr+=1\n",
    "\n",
    "\n",
    "        if timer >= set_timer_start and no_movement_ctr >=between_exercise_ctr:\n",
    "            cv2.putText(frame, \"PERFORM THE EXERCISE\", (180, 460), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 150, 255), 2)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "# ====================================================(left_arm)=========================================================================\n",
    "# --------------------------------------------------(left_upper_arm)-----------------------------------------------------------------------\n",
    "    correct_threshold_color_modifier = .25\n",
    "    if left_upper_arm_compare >= correct_threshold:\n",
    "        color_percentage_modifier = (left_upper_arm_compare - correct_threshold-correct_threshold_color_modifier) / (1 - correct_threshold-correct_threshold_color_modifier ) * 100          \n",
    "        draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(int(255*color_percentage_modifier), 0,0 ))        \n",
    "    elif left_upper_arm_compare <= correct_threshold: \n",
    "        draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(0, 0, 255))\n",
    "    else: \n",
    "        draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(0, 255, 0))\n",
    "# ---------------------------------------------------(left_lower_arm)----------------------------------------------------------------------\n",
    "    if left_lower_arm_compare >= correct_threshold: \n",
    "        color_percentage_modifier = (left_lower_arm_compare - correct_threshold-correct_threshold_color_modifier) / (1 - correct_threshold-correct_threshold_color_modifier ) * 100    \n",
    "        draw_pose_lines(frame,[prev_array[13][0],prev_array[13][1],prev_array[15][0],prev_array[15][1]],(int(255*color_percentage_modifier), 0,0 ))\n",
    "    elif left_lower_arm_compare <= correct_threshold: \n",
    "        draw_pose_lines(frame,[prev_array[13][0],prev_array[13][1],prev_array[15][0],prev_array[15][1]],(0, 0, 255))\n",
    "    else: \n",
    "        draw_pose_lines(frame,[prev_array[13][0],prev_array[13][1],prev_array[15][0],prev_array[15][1]],(0, 255, 0))\n",
    "# ----------------------------------------------------(left_hand)---------------------------------------------------------------------\n",
    "    # if left_hand_temp_compare >= correct_threshold:  \n",
    "        # draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]])\n",
    "        # wrong_activator_line[2]=0\n",
    "    # elif left_hand_temp_compare <= correct_threshold: \n",
    "    #     draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(0, 0, 255))\n",
    "    # wrong_activator_line[2]=0\n",
    "    # else: \n",
    "    #     draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(255, 0, 0))\n",
    "# =============================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================(right_arm)==============================================================================\n",
    "# --------------------------------------------------(right_upper_arm)-----------------------------------------------------------------------\n",
    "    if right_upper_arm_compare >= correct_threshold: \n",
    "        color_percentage_modifier = (right_upper_arm_compare - correct_threshold-correct_threshold_color_modifier) / (1 - correct_threshold-correct_threshold_color_modifier ) * 100  \n",
    "        draw_pose_lines(frame,[prev_array[12][0],prev_array[12][1],prev_array[14][0],prev_array[14][1]],(255,int(255*color_percentage_modifier),0 ))\n",
    "    elif right_upper_arm_compare <= correct_threshold: \n",
    "        draw_pose_lines(frame,[prev_array[12][0],prev_array[12][1],prev_array[14][0],prev_array[14][1]],(0, 0, 255))\n",
    "    else: \n",
    "        draw_pose_lines(frame,[prev_array[12][0],prev_array[12][1],prev_array[14][0],prev_array[14][1]],(0, 255, 0))\n",
    "# ----------------------------------------------------(right_lower_arm---------------------------------------------------------------------\n",
    "    if right_lower_arm_compare >= correct_threshold:\n",
    "        color_percentage_modifier = (right_lower_arm_compare - correct_threshold-correct_threshold_color_modifier) / (1 - correct_threshold-correct_threshold_color_modifier ) * 100  \n",
    "        draw_pose_lines(frame,[prev_array[14][0],prev_array[14][1],prev_array[16][0],prev_array[16][1]],(255, int(255*color_percentage_modifier),0 ))\n",
    "    elif right_lower_arm_compare <= correct_threshold:\n",
    "        draw_pose_lines(frame,[prev_array[14][0],prev_array[14][1],prev_array[16][0],prev_array[16][1]],(0, 0, 255))\n",
    "    else: \n",
    "        draw_pose_lines(frame,[prev_array[14][0],prev_array[14][1],prev_array[16][0],prev_array[16][1]],(0, 255, 0))\n",
    "# -----------------------------------------------------(right_hand)--------------------------------------------------------------------\n",
    "    # if right_hand_compare >= correct_threshold:\n",
    "        # draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]])\n",
    "        # wrong_activator_line[5]=0\n",
    "    # elif right_hand_compare <= correct_threshold:\n",
    "    #     draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(0, 0, 255))\n",
    "        # wrong_activator_line[5]=0\n",
    "    # else: \n",
    "    #     draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(255, 0, 0))\n",
    "# =============================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =================================================(left_leg)============================================================================\n",
    "# ----------------------------------------------------(left_upper_leg)---------------------------------------------------------------------\n",
    "    if left_upper_leg_compare >= correct_threshold:\n",
    "        color_percentage_modifier = (left_upper_leg_compare - correct_threshold-correct_threshold_color_modifier) / (1 - correct_threshold-correct_threshold_color_modifier ) * 100  \n",
    "        draw_pose_lines(frame,[prev_array[23][0],prev_array[23][1],prev_array[25][0],prev_array[25][1]],(255,int(255*color_percentage_modifier),0 ))\n",
    "    elif left_upper_leg_compare <= correct_threshold:\n",
    "        draw_pose_lines(frame,[prev_array[23][0],prev_array[23][1],prev_array[25][0],prev_array[25][1]],(0, 0, 255))\n",
    "    else: \n",
    "        draw_pose_lines(frame,[prev_array[23][0],prev_array[23][1],prev_array[25][0],prev_array[25][1]],(0, 255, 0))\n",
    "# -----------------------------------------------------(left_lower_leg)--------------------------------------------------------------------\n",
    "    if left_lower_leg_compare >= correct_threshold:\n",
    "        color_percentage_modifier = (left_lower_leg_compare - correct_threshold-correct_threshold_color_modifier) / (1 - correct_threshold-correct_threshold_color_modifier ) * 100  \n",
    "        draw_pose_lines(frame,[prev_array[25][0],prev_array[25][1],prev_array[27][0],prev_array[27][1]],(255,int(255*color_percentage_modifier),0 ))\n",
    "    elif left_lower_leg_compare <= correct_threshold:\n",
    "        draw_pose_lines(frame,[prev_array[25][0],prev_array[25][1],prev_array[27][0],prev_array[27][1]],(0, 0, 255))\n",
    "    else: \n",
    "        draw_pose_lines(frame,[prev_array[25][0],prev_array[25][1],prev_array[27][0],prev_array[27][1]],(0, 255, 0))\n",
    "# -------------------------------------------------------(left_feet)------------------------------------------------------------------\n",
    "    # if left_feet_compare >= correct_threshold:\n",
    "        # draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]])\n",
    "    # elif left_feet_compare <= correct_threshold:\n",
    "    #     draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(0, 0, 255))\n",
    "    # else: \n",
    "    #     draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(255, 0, 0))\n",
    "# =============================================================================================================================\n",
    "\n",
    "\n",
    "# =======================================================(right_leg)======================================================================\n",
    "# ----------------------------------------------------(left_upper_leg)---------------------------------------------------------------------\n",
    "    if right_upper_leg_compare >= correct_threshold:\n",
    "        color_percentage_modifier = (right_upper_leg_compare - correct_threshold-correct_threshold_color_modifier) / (1 - correct_threshold-correct_threshold_color_modifier ) * 100  \n",
    "        draw_pose_lines(frame,[prev_array[24][0],prev_array[24][1],prev_array[26][0],prev_array[26][1]],(255,int(255*color_percentage_modifier),0 ))\n",
    "    elif right_upper_leg_compare <= correct_threshold:\n",
    "        draw_pose_lines(frame,[prev_array[24][0],prev_array[24][1],prev_array[26][0],prev_array[26][1]],(0, 0, 255))\n",
    "    else: \n",
    "        draw_pose_lines(frame,[prev_array[24][0],prev_array[24][1],prev_array[26][0],prev_array[26][1]],(0, 255, 0))\n",
    "# ----------------------------------------------------(left_upper_leg)---------------------------------------------------------------------\n",
    "    if right_lower_leg_compare >= correct_threshold:\n",
    "        color_percentage_modifier = (right_lower_leg_compare - correct_threshold-correct_threshold_color_modifier) / (1 - correct_threshold-correct_threshold_color_modifier ) * 100  \n",
    "        draw_pose_lines(frame,[prev_array[26][0],prev_array[26][1],prev_array[28][0],prev_array[28][1]],(255,int(255*color_percentage_modifier),0 ))\n",
    "    elif right_lower_leg_compare <= correct_threshold:\n",
    "        draw_pose_lines(frame,[prev_array[26][0],prev_array[26][1],prev_array[28][0],prev_array[28][1]],(0, 0, 255))\n",
    "    else: \n",
    "        draw_pose_lines(frame,[prev_array[26][0],prev_array[26][1],prev_array[28][0],prev_array[28][1]],(0, 255, 0))\n",
    "# ----------------------------------------------------(left_upper_leg)---------------------------------------------------------------------\n",
    "    # if right_feet_compare >= correct_threshold:\n",
    "    #     # draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]])\n",
    "    #   wrong_activator_line[3]=0\n",
    "    # elif right_feet_compare <= correct_threshold:\n",
    "    #     draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(0, 0, 255))\n",
    "    #       wrong_activator_line[3]=0\n",
    "    # else: \n",
    "    #     draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]],(255, 0, 0))\n",
    "# =============================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================================================================   \n",
    "    # if body_temp_compare >= correct_threshold:\n",
    "        # draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]])\n",
    "\n",
    "    # if head_temp_compare >= correct_threshold:\n",
    "        # draw_pose_lines(frame,[prev_array[11][0],prev_array[11][1],prev_array[13][0],prev_array[13][1]])\n",
    "# =============================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "#Outputing important informations-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        cv2.putText(frame, str(movement_boolean), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"no_movement_ctr-> {no_movement_ctr}\", (20, 37), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, str(landmark_all_present), (20, 54), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"timer-> {timer}\", (20, 73), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"exercise exectuted-> {exercise_exectuted_ctr}\", (20, 92), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        if timer < set_timer_start:\n",
    "            cv2.putText(frame, \"PREPARE YOUR STARTING POSE\", (180, 460), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n",
    "            \n",
    "\n",
    "\n",
    "        # mp_drawing.draw_landmarks(\n",
    "        #     frame,\n",
    "        #     results.pose_landmarks,\n",
    "        #     mp_pose.POSE_CONNECTIONS,\n",
    "        #     landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "        #     connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "        # )\n",
    "\n",
    "    cv2.imshow('MediaPipe Pose', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if temp = 0 >=90:\n",
    "#     print('yesysey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=[2,3,4,5,6,7]\n",
    "# temp = np.array(temp)\n",
    "# print(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inferencing\n",
    "\n",
    "\n",
    "\n",
    "# mp_pose = mp.solutions.pose\n",
    "# pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # cap = cv2.VideoCapture(\"vid4.mp4\")\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# frame_ctr = 0\n",
    "# frame_ctr_threshold = 0\n",
    "# exercise_ctr = 0\n",
    "# prev_prediction = 0\n",
    "# output_data = 0\n",
    "# temp_checking = []\n",
    "# all_array = []\n",
    "# # it means it is waiting to reset\n",
    "# prediction_buffer_threshold = .05\n",
    "# model = tf.lite.Interpreter(model_path=\"D:\\CLARK\\Documents\\RNN\\RNN_proofOfConcept\\converted_model_9683default3.tflite\")\n",
    "# model.allocate_tensors()\n",
    "# input_details = model.get_input_details()\n",
    "# output_details = model.get_output_details()\n",
    "# input_shape = input_details[0]['shape']\n",
    "# # print('input ->',)\n",
    "\n",
    "# prediction =[]\n",
    "# one_exercise = []\n",
    "# # max length of sequence\n",
    "# for y in range(input_shape[1]):\n",
    "#     sequence_input=[]\n",
    "#     # max num of landmark x and y flattened\n",
    "#     for x in range(66):\n",
    "#         sequence_input.append(0)\n",
    "#     one_exercise.append(sequence_input)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     frame_ctr += 1\n",
    "#     # print(frame_ctr)\n",
    "#     ret, frame = cap.read()\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = pose.process(image_rgb)\n",
    "\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     if results.pose_landmarks:\n",
    "#         temp_sequence=[]\n",
    "#         for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "#             x = landmark.x\n",
    "#             y = landmark.y\n",
    "#             # print(idx,\")\",x,\"---\",prev_array[idx][0],)\n",
    "#             # print(idx)\n",
    "#             # x = round(float(x),4)\n",
    "#             # y = round(float(y),4)\n",
    "#             temp_sequence.append(x)\n",
    "#             temp_sequence.append(y)\n",
    "#         one_exercise.append(temp_sequence)\n",
    "#         print(one_exercise[0][0],'------',temp_sequence[0])\n",
    "        \n",
    "\n",
    "#     # if max length of the exercise sequence was reached then pop it pop off\n",
    "#         if len(one_exercise) >= input_shape[1]:\n",
    "#             one_exercise.pop(0)\n",
    "\n",
    "#         all_array.append(one_exercise[:])\n",
    "        \n",
    "#         if frame_ctr >= frame_ctr_threshold:            \n",
    "#             frame_ctr = 0\n",
    "            \n",
    "#             prediction = np.array(one_exercise)\n",
    "#             prediction = prediction.astype(np.float32)\n",
    "#             prediction = prediction.reshape(1,input_shape[1],66)\n",
    "#             # current_prediction = model.predict(prediction)[0][0]\n",
    "#             # print(prediction)\n",
    "#             model.set_tensor(input_details[0]['index'], prediction)\n",
    "#             model.invoke()\n",
    "#             output_data = model.get_tensor(output_details[0]['index'])\n",
    "#             # predicted_class = np.argmax(output_data)\n",
    "\n",
    "\n",
    "#             # print(output_data)\n",
    "#             if output_data >=.89:\n",
    "#                 exercise_ctr +=1\n",
    "#                 temp_checking.append(one_exercise)\n",
    "\n",
    "#                 # current_prediction = output_data\n",
    "#                 # if prev_prediction + prediction_buffer_threshold >= current_prediction and prev_prediction - prediction_buffer_threshold <= current_prediction:\n",
    "#                 #     None\n",
    "#                 # else:\n",
    "#                 #     exercise_ctr +=1\n",
    "\n",
    "#                 # prev_prediction = current_prediction                \n",
    "                \n",
    "\n",
    "#             # interpreter.reset_all_variables()\n",
    "        \n",
    "\n",
    "\n",
    "#         # interpreter.set_tensor(input_details[0]['index'], [prediction])\n",
    "#         # interpreter.invoke()\n",
    "#         # output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "#         # predicted_class = np.argmax(output_data)\n",
    "#         # print(\"Predicted Class:\", predicted_class)\n",
    "#         # print(\"Predicted Class:\", output_data)\n",
    "\n",
    "        \n",
    "#         # print(len(one_exercise))\n",
    "#         # print(prediction.shape)\n",
    "#         # \n",
    "#         # \n",
    "#         # if model.predict(prediction)[0][0]>=.8:\n",
    "#         #     print('correct')\n",
    "#     cv2.putText(frame, str(exercise_ctr), (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "#     cv2.putText(frame, str(output_data), (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         frame,\n",
    "#         results.pose_landmarks,\n",
    "#         mp_pose.POSE_CONNECTIONS,\n",
    "#         landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "#         connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "#     )\n",
    "\n",
    "#     cv2.imshow('MediaPipe Pose', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "\n",
    "# # Initialize MediaPipe Pose model\n",
    "# mp_pose = mp.solutions.pose\n",
    "# pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# # Start video capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Process the frame with MediaPipe\n",
    "#     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = pose.process(frame_rgb)\n",
    "\n",
    "#     # Draw only specific connections and landmarks\n",
    "#     if results.pose_landmarks:\n",
    "#         # Draw specific connections (e.g., from left shoulder to left wrist)\n",
    "#         specific_connections = [(mp_pose.PoseLandmark.LEFT_SHOULDER.value, mp_pose.PoseLandmark.LEFT_WRIST.value)]\n",
    "#         for connection in specific_connections:\n",
    "#             idx_start, idx_end = connection\n",
    "#             start_point = (int(results.pose_landmarks.landmark[idx_start].x * frame.shape[1]),\n",
    "#                            int(results.pose_landmarks.landmark[idx_start].y * frame.shape[0]))\n",
    "#             end_point = (int(results.pose_landmarks.landmark[idx_end].x * frame.shape[1]),\n",
    "#                          int(results.pose_landmarks.landmark[idx_end].y * frame.shape[0]))\n",
    "#             cv2.line(frame, start_point, end_point, (0, 0, 255), 2)\n",
    "\n",
    "#         # Draw specific landmarks (e.g., left wrist)\n",
    "#         specific_landmarks = [mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "#         for landmark in specific_landmarks:\n",
    "#             idx = landmark.value\n",
    "#             center_point = (int(results.pose_landmarks.landmark[idx].x * frame.shape[1]),\n",
    "#                             int(results.pose_landmarks.landmark[idx].y * frame.shape[0]))\n",
    "#             cv2.circle(frame, center_point, radius=2, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "#     # Display the frame with specific connections and landmarks\n",
    "#     cv2.imshow('Customized Frame', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
